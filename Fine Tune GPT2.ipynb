{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZMbGCw0Qxfg"
   },
   "source": [
    "# **Fine-tuning GPT2 for text generation**\n",
    "### This notebook has been taken from [here](https://gist.github.com/GeorgeDittmar/5c57a35332b2b5818e51618af7953351)\n",
    "- It contains code to fine tune GPT-2 on two datasets:\n",
    "  1. [Tiny Shakespeare](https://huggingface.co/datasets/tiny_shakespeare) - 40,000 lines of Shakespeare from a variety of Shakespeare's plays\n",
    "  2. [Bill Sum](https://huggingface.co/datasets/billsum) - Summarization of US Congressional and California state bills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCoFXKmRS2ru"
   },
   "source": [
    "## Table of contents\n",
    "1. [Imports and Installation section](#Imports)\n",
    "2. [Shakespeare Data preparation](#Shakespeare)\n",
    "3. [GPT-2 Model Fine tuning on Shakespeare Dataset](#FineTuning)\n",
    "4. [Shakespeare Text Generation using GPT-2](#TextGeneration) \n",
    "5. [Bill Sum Data set preparation](#BillSum)\n",
    "6. [GPT-2 Model Fine tuning on Bill Sum Dataset](#FineTuningBillSum)\n",
    "7. [Bill Sum Text Generation using GPT-2](#BillSumTextGeneration) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0DeILhoTAVZ"
   },
   "source": [
    "## Part 1: Imports and Installation section <a name=\"Imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "le99a8fWkpoX"
   },
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B2z-X4SYkwE8",
    "outputId": "57846aa6-6d08-49a3-ab22-5b6b6e8eb887"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "91rmSAUQVIUP",
    "outputId": "edcedec0-a853-42cf-de84-a8a59d1dda90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'transformers'...\n",
      "remote: Enumerating objects: 19, done.\u001b[K\n",
      "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
      "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
      "remote: Total 62213 (delta 3), reused 9 (delta 0), pack-reused 62194\u001b[K\n",
      "Receiving objects: 100% (62213/62213), 47.44 MiB | 28.97 MiB/s, done.\n",
      "Resolving deltas: 100% (44035/44035), done.\n"
     ]
    }
   ],
   "source": [
    "#Clone the transformers repo into the notebook\n",
    "!git clone https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFzEr4Y1VJKP",
    "outputId": "63d4586d-d0cb-4ed1-f21a-0116862ee053"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data  transformers\n"
     ]
    }
   ],
   "source": [
    "# Clone should now be in the machine\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOTdb4rWv8YN"
   },
   "source": [
    "Change directory location to be in the examples folder and then install any requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2M2Oz9CYB4P",
    "outputId": "b3e606e3-c581-4e60-9a8e-51bebd214f0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t  run_clm.py\t   run_mlm.py\n",
      "requirements.txt  run_mlm_flax.py  run_plm.py\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"transformers\")\n",
    "os.chdir(\"./examples/language-modeling\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "04rBGxwiYnep",
    "outputId": "7f44c125-0afa-456a-9389-69e1ce0f6e07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets>=1.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/9b/d097f2238fc3c028495cf5f8c65378972b9f1b2cbb27f3c57c7219195aa9/datasets-1.2.1-py3-none-any.whl (159kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 17.1MB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 47.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (3.12.4)\n",
      "Collecting pyarrow>=0.17.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/67/2f4fcce1b41bcc7e88a6bfdb42046597ae72e5bc95c2789b7c5ac893c433/pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7MB)\n",
      "\u001b[K     |████████████████████████████████| 20.7MB 1.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (4.41.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (2.23.0)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 61.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.70.11.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (1.1.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets>=1.1.3->-r requirements.txt (line 1)) (0.3.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->-r requirements.txt (line 3)) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->-r requirements.txt (line 3)) (53.0.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Installing collected packages: pyarrow, xxhash, datasets, sentencepiece\n",
      "  Found existing installation: pyarrow 0.14.1\n",
      "    Uninstalling pyarrow-0.14.1:\n",
      "      Successfully uninstalled pyarrow-0.14.1\n",
      "Successfully installed datasets-1.2.1 pyarrow-3.0.0 sentencepiece-0.1.95 xxhash-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iB29mAKjaNIQ",
    "outputId": "acfe161c-cfff-4de1-b44f-86256bed26d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\t  run_clm.py\t   run_mlm.py\n",
      "requirements.txt  run_mlm_flax.py  run_plm.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eo5gRmXaWx0m",
    "outputId": "7b53d919-a8cd-44a4-f6bc-d1485af8b19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pyarrow in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y5sdYSpAWY1S"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/transformers/examples/\")\n",
    "os.chdir(\"./language-modeling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6G6WINaYmwx",
    "outputId": "bb2dab56-fac3-4d8d-ea0f-180a8251c5d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/huggingface/transformers/\n",
      "  Cloning git://github.com/huggingface/transformers/ to /tmp/pip-req-build-txc9i948\n",
      "  Running command git clone -q git://github.com/huggingface/transformers/ /tmp/pip-req-build-txc9i948\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (20.9)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.4.0.dev0) (0.8)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 16.9MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 59.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.0)\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for transformers: filename=transformers-4.4.0.dev0-cp36-none-any.whl size=1808787 sha256=1977580b98abbeaf11ff53ffbeb24ea299c7950511eca6740b420fb5ea1a4e2f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4p5t8ob0/wheels/dc/e5/1e/3a2977a646558fca07585cadcf56aa4a910e995ba945961c4e\n",
      "Successfully built transformers\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2b8048d1b925501e3ea9b7ac81da1ab91eef176a61fbad92c3116bdb59c97c4d\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Need to install latest transformer packages from github so the scripts will run correctly\n",
    "!pip install git+git://github.com/huggingface/transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdWf9TULTUnC"
   },
   "source": [
    "## Part 2: Shakespeare Data Preparation section<a name=\"Shakespeare\"></a>\n",
    "- Set up data from a text file in the format <|title|> some data <|endoftext|> and split into training and eval sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uU4ckPTf9T-2",
    "outputId": "f1be4580-36a7-4e18-ede1-5f58ac43092c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:36000\n",
      "Evaluation size: 4000\n"
     ]
    }
   ],
   "source": [
    "with open('/content/drive/MyDrive/bill_sum_shakespeare/shakespeare.txt', 'r') as data:\n",
    "  dataset = [\"<|title|>\" + x.strip() for x in data.readlines()]\n",
    "\n",
    "train, eval = train_test_split(dataset, train_size = 0.9, random_state = 42)\n",
    "print(\"Training size:\" + str(len(train)))\n",
    "print(\"Evaluation size: \" + str(len(eval)))\n",
    "\n",
    "with open('train_tmp.txt', 'w') as file_handle:\n",
    "  file_handle.write(\"<|endoftext|>\".join(train))\n",
    "\n",
    "with open('eval_tmp.txt', 'w') as file_handle:\n",
    "  file_handle.write(\"<|endoftext|>\".join(eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TYi8Oqs6Npo"
   },
   "source": [
    "## Part 3: GPT-2 Model Fine tuning on Shakespeare Dataset<a name=\"FineTuning\"></a>\n",
    "### Fine tuning with **1 epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyV_rTL3ZE_H",
    "outputId": "9b486ab3-8276-4af7-f3a5-ba3a4ffda41c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-10 14:01:11.237413: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/10/2021 14:01:12 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/10/2021 14:01:12 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Output dir>, overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Feb10_14-01-12_cf74782d0fe2, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Downloading: 2.57kB [00:00, 3.26MB/s]       \n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset text/default-beef45babbf5d574 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-beef45babbf5d574/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
      "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-beef45babbf5d574/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
      "[INFO|file_utils.py:1302] 2021-02-10 14:01:12,866 >> https://huggingface.co/gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpo82nusrj\n",
      "02/10/2021 14:01:12 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpo82nusrj\n",
      "Downloading: 100% 718/718 [00:00<00:00, 1.06MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 14:01:12,968 >> storing https://huggingface.co/gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 14:01:12 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|file_utils.py:1309] 2021-02-10 14:01:12,968 >> creating metadata file for /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 14:01:12 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 14:01:12,968 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 14:01:12 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 14:01:12,969 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 14:01:12 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 14:01:12,983 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 14:01:12 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 14:01:12,984 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 14:01:12 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:1302] 2021-02-10 14:01:13,008 >> https://huggingface.co/gpt2-medium/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplchyp_xb\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplchyp_xb\n",
      "Downloading: 100% 1.04M/1.04M [00:00<00:00, 29.6MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 14:01:13,067 >> storing https://huggingface.co/gpt2-medium/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|file_utils.py:1309] 2021-02-10 14:01:13,067 >> creating metadata file for /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|file_utils.py:1302] 2021-02-10 14:01:13,088 >> https://huggingface.co/gpt2-medium/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpiwx0wg34\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpiwx0wg34\n",
      "Downloading: 100% 456k/456k [00:00<00:00, 22.6MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 14:01:13,127 >> storing https://huggingface.co/gpt2-medium/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|file_utils.py:1309] 2021-02-10 14:01:13,127 >> creating metadata file for /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|file_utils.py:1302] 2021-02-10 14:01:13,153 >> https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu39ynqhe\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu39ynqhe\n",
      "Downloading: 100% 1.36M/1.36M [00:00<00:00, 37.9MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 14:01:13,214 >> storing https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|file_utils.py:1309] 2021-02-10 14:01:13,214 >> creating metadata file for /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:01:13,214 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 14:01:13 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:01:13,214 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 14:01:13 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 14:01:13,214 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 14:01:13 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|file_utils.py:1302] 2021-02-10 14:01:13,298 >> https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprchfwg54\n",
      "02/10/2021 14:01:13 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprchfwg54\n",
      "Downloading: 100% 1.52G/1.52G [00:18<00:00, 82.8MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 14:01:31,756 >> storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 14:01:31 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|file_utils.py:1309] 2021-02-10 14:01:31,756 >> creating metadata file for /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 14:01:31 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 14:01:31,757 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 14:01:31 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 14:01:44,075 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "02/10/2021 14:01:44 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 14:01:44,075 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "02/10/2021 14:01:44 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-10 14:01:45,143 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "02/10/2021 14:01:45 - WARNING - transformers.tokenization_utils_base -   Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100% 1/1 [00:01<00:00,  1.06s/ba]\n",
      "100% 1/1 [00:00<00:00,  9.68ba/s]\n",
      "100% 1/1 [00:00<00:00,  2.89ba/s]\n",
      "100% 1/1 [00:00<00:00, 27.66ba/s]\n",
      "[INFO|trainer.py:432] 2021-02-10 14:02:01,938 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-10 14:02:01,939 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-10 14:02:01,939 >> Using amp fp16 backend\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -   Using amp fp16 backend\n",
      "[WARNING|training_args.py:514] 2021-02-10 14:02:01,940 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 14:02:01 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:514] 2021-02-10 14:02:01,945 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 14:02:01 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-10 14:02:01,945 >> ***** Running training *****\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -   ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 14:02:01,945 >>   Num examples = 472\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -     Num examples = 472\n",
      "[INFO|trainer.py:839] 2021-02-10 14:02:01,945 >>   Num Epochs = 1\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -     Num Epochs = 1\n",
      "[INFO|trainer.py:840] 2021-02-10 14:02:01,945 >>   Instantaneous batch size per device = 8\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-10 14:02:01,945 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-10 14:02:01,945 >>   Gradient Accumulation steps = 1\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-10 14:02:01,945 >>   Total optimization steps = 472\n",
      "02/10/2021 14:02:01 - INFO - transformers.trainer -     Total optimization steps = 472\n",
      "[WARNING|training_args.py:514] 2021-02-10 14:02:01,955 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 14:02:01 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/472 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "100% 472/472 [05:17<00:00,  1.47it/s][INFO|trainer.py:1011] 2021-02-10 14:07:19,323 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "02/10/2021 14:07:19 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 317.3779, 'train_samples_per_second': 1.487, 'epoch': 1.0}\n",
      "100% 472/472 [05:17<00:00,  1.49it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-10 14:07:19,325 >> Saving model checkpoint to <Output dir>\n",
      "02/10/2021 14:07:19 - INFO - transformers.trainer -   Saving model checkpoint to <Output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-10 14:07:19,327 >> Configuration saved in <Output dir>/config.json\n",
      "02/10/2021 14:07:19 - INFO - transformers.configuration_utils -   Configuration saved in <Output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-10 14:07:24,738 >> Model weights saved in <Output dir>/pytorch_model.bin\n",
      "02/10/2021 14:07:24 - INFO - transformers.modeling_utils -   Model weights saved in <Output dir>/pytorch_model.bin\n",
      "02/10/2021 14:07:24 - INFO - __main__ -   ***** Train results *****\n",
      "02/10/2021 14:07:24 - INFO - __main__ -     epoch = 1.0\n",
      "02/10/2021 14:07:24 - INFO - __main__ -     train_runtime = 317.3779\n",
      "02/10/2021 14:07:24 - INFO - __main__ -     train_samples_per_second = 1.487\n",
      "02/10/2021 14:07:24 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-10 14:07:24,812 >> ***** Running Evaluation *****\n",
      "02/10/2021 14:07:24 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 14:07:24,812 >>   Num examples = 52\n",
      "02/10/2021 14:07:24 - INFO - transformers.trainer -     Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-10 14:07:24,813 >>   Batch size = 8\n",
      "02/10/2021 14:07:24 - INFO - transformers.trainer -     Batch size = 8\n",
      "100% 7/7 [00:15<00:00,  2.27s/it]\n",
      "02/10/2021 14:07:44 - INFO - __main__ -   ***** Eval results *****\n",
      "02/10/2021 14:07:44 - INFO - __main__ -     perplexity = 10.142024001583698\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 1 \\\n",
    "--fp16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--output_dir=\"Output dir\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mctfrJ65FOY"
   },
   "source": [
    "### Fine tuning with **3 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTVGqlyd5ITu",
    "outputId": "0d6ead37-066d-4706-81d4-ab3f2fe8b253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 10:27:58.071755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/09/2021 10:27:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/09/2021 10:27:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb09_10-27-59_aa1c1fd0c52e, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 10:27:59,553 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 10:27:59,553 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 10:27:59,567 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 10:27:59,568 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 10:27:59,638 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 10:27:59,638 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 10:27:59,638 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-09 10:27:59,723 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-09 10:28:11,757 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-09 10:28:11,757 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-09 10:28:12,820 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-87250f9a92327413.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-738404bd7c994409.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-432212c93cdcf829.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-09d4cdd226cb0aca.arrow\n",
      "[INFO|trainer.py:432] 2021-02-09 10:28:18,608 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-09 10:28:18,609 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-09 10:28:18,609 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:502] 2021-02-09 10:28:18,610 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:502] 2021-02-09 10:28:18,615 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-09 10:28:18,615 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-09 10:28:18,615 >>   Num examples = 472\n",
      "[INFO|trainer.py:839] 2021-02-09 10:28:18,615 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:840] 2021-02-09 10:28:18,615 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-09 10:28:18,615 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-09 10:28:18,615 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-09 10:28:18,615 >>   Total optimization steps = 1416\n",
      "[WARNING|training_args.py:502] 2021-02-09 10:28:19,709 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/1416 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 33% 472/1416 [05:16<10:41,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 10:33:35,984 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 10:33:35,984 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 10:33:35,985 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 43% 3/7 [00:02<00:03,  1.03it/s]\u001b[A\n",
      " 57% 4/7 [00:05<00:04,  1.54s/it]\u001b[A\n",
      " 71% 5/7 [00:08<00:03,  1.95s/it]\u001b[A\n",
      " 86% 6/7 [00:11<00:02,  2.23s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.320230007171631, 'eval_runtime': 18.9235, 'eval_samples_per_second': 2.748, 'epoch': 1.0}\n",
      " 33% 472/1416 [05:35<10:41,  1.47it/s]\n",
      "100% 7/7 [00:15<00:00,  2.42s/it]\u001b[A\n",
      "{'loss': 2.4222, 'learning_rate': 5e-05, 'epoch': 1.06}\n",
      " 67% 944/1416 [10:55<05:20,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 10:39:15,236 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 10:39:15,236 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 10:39:15,236 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.51s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.2977471351623535, 'eval_runtime': 18.5855, 'eval_samples_per_second': 2.798, 'epoch': 2.0}\n",
      " 67% 944/1416 [11:14<05:20,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 2.1414, 'learning_rate': 5e-05, 'epoch': 2.12}\n",
      "100% 1416/1416 [16:33<00:00,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 10:44:53,645 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 10:44:53,645 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 10:44:53,645 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.325777053833008, 'eval_runtime': 18.6678, 'eval_samples_per_second': 2.786, 'epoch': 3.0}\n",
      "100% 1416/1416 [16:52<00:00,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.63s/it]\u001b[A\n",
      "                                 \u001b[A[INFO|trainer.py:1011] 2021-02-09 10:45:12,314 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1013.6984, 'train_samples_per_second': 1.397, 'epoch': 3.0}\n",
      "100% 1416/1416 [16:52<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-09 10:45:12,315 >> Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-09 10:45:12,318 >> Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-09 10:45:19,353 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/09/2021 10:45:19 - INFO - __main__ -   ***** Train results *****\n",
      "02/09/2021 10:45:19 - INFO - __main__ -     epoch = 3.0\n",
      "02/09/2021 10:45:19 - INFO - __main__ -     train_runtime = 1013.6984\n",
      "02/09/2021 10:45:19 - INFO - __main__ -     train_samples_per_second = 1.397\n",
      "02/09/2021 10:45:19 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-09 10:45:19,434 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 10:45:19,434 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 10:45:19,434 >>   Batch size = 8\n",
      "100% 7/7 [00:18<00:00,  2.65s/it]\n",
      "02/09/2021 10:45:38 - INFO - __main__ -   ***** Eval results *****\n",
      "02/09/2021 10:45:38 - INFO - __main__ -     perplexity = 10.23462985504993\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 3 \\\n",
    "--fp16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPFqA6NnZ_fM"
   },
   "source": [
    "### Fine tuning with **5 epochs**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBa9HkqmaEsB",
    "outputId": "8ec8385d-96f8-48c8-b932-109a9d78b10c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 10:48:48.805022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/09/2021 10:48:50 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/09/2021 10:48:50 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb09_10-48-50_aa1c1fd0c52e, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 10:48:50,276 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 10:48:50,276 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 10:48:50,292 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 10:48:50,292 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 10:48:50,366 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 10:48:50,366 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 10:48:50,366 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-09 10:48:50,446 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-09 10:49:02,575 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-09 10:49:02,575 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-09 10:49:03,619 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-87250f9a92327413.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-738404bd7c994409.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-432212c93cdcf829.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-09d4cdd226cb0aca.arrow\n",
      "[INFO|trainer.py:432] 2021-02-09 10:49:09,467 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-09 10:49:09,468 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-09 10:49:09,468 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:502] 2021-02-09 10:49:09,469 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:502] 2021-02-09 10:49:09,474 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-09 10:49:09,474 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-09 10:49:09,474 >>   Num examples = 472\n",
      "[INFO|trainer.py:839] 2021-02-09 10:49:09,474 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:840] 2021-02-09 10:49:09,474 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-09 10:49:09,474 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-09 10:49:09,474 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-09 10:49:09,474 >>   Total optimization steps = 2360\n",
      "[WARNING|training_args.py:502] 2021-02-09 10:49:10,399 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/2360 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 20% 472/2360 [05:17<21:23,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 10:54:27,811 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 10:54:27,811 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 10:54:27,812 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 43% 3/7 [00:02<00:03,  1.03it/s]\u001b[A\n",
      " 57% 4/7 [00:05<00:04,  1.54s/it]\u001b[A\n",
      " 71% 5/7 [00:08<00:03,  1.94s/it]\u001b[A\n",
      " 86% 6/7 [00:11<00:02,  2.22s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.320230007171631, 'eval_runtime': 18.881, 'eval_samples_per_second': 2.754, 'epoch': 1.0}\n",
      " 20% 472/2360 [05:36<21:23,  1.47it/s]\n",
      "100% 7/7 [00:15<00:00,  2.42s/it]\u001b[A\n",
      "{'loss': 2.4222, 'learning_rate': 5e-05, 'epoch': 1.06}\n",
      " 40% 944/2360 [10:56<16:01,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 11:00:06,657 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:00:06,657 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:00:06,657 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.2977471351623535, 'eval_runtime': 18.5893, 'eval_samples_per_second': 2.797, 'epoch': 2.0}\n",
      " 40% 944/2360 [11:14<16:01,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 2.1414, 'learning_rate': 5e-05, 'epoch': 2.12}\n",
      " 60% 1416/2360 [16:34<10:38,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 11:05:45,399 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:05:45,399 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:05:45,399 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.325777053833008, 'eval_runtime': 18.643, 'eval_samples_per_second': 2.789, 'epoch': 3.0}\n",
      " 60% 1416/2360 [16:53<10:38,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 1.9732, 'learning_rate': 5e-05, 'epoch': 3.18}\n",
      " 80% 1888/2360 [22:13<05:20,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 11:11:23,832 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:11:23,832 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:11:23,832 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.53s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.3956875801086426, 'eval_runtime': 18.7006, 'eval_samples_per_second': 2.781, 'epoch': 4.0}\n",
      " 80% 1888/2360 [22:32<05:20,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.63s/it]\u001b[A\n",
      "{'loss': 1.8145, 'learning_rate': 5e-05, 'epoch': 4.24}\n",
      "100% 2360/2360 [27:51<00:00,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 11:17:02,389 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:17:02,389 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:17:02,389 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "100% 7/7 [00:17<00:00,  2.62s/it]\u001b[A\n",
      "{'eval_loss': 2.482832670211792, 'eval_runtime': 18.6254, 'eval_samples_per_second': 2.792, 'epoch': 5.0}\n",
      "\n",
      "100% 2360/2360 [28:10<00:00,  1.47it/s]\n",
      "                                 \u001b[A[INFO|trainer.py:1011] 2021-02-09 11:17:21,015 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1691.5413, 'train_samples_per_second': 1.395, 'epoch': 5.0}\n",
      "100% 2360/2360 [28:10<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-09 11:17:21,017 >> Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-09 11:17:21,020 >> Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-09 11:17:27,180 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/09/2021 11:17:27 - INFO - __main__ -   ***** Train results *****\n",
      "02/09/2021 11:17:27 - INFO - __main__ -     epoch = 5.0\n",
      "02/09/2021 11:17:27 - INFO - __main__ -     train_runtime = 1691.5413\n",
      "02/09/2021 11:17:27 - INFO - __main__ -     train_samples_per_second = 1.395\n",
      "02/09/2021 11:17:27 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-09 11:17:27,260 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:17:27,260 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:17:27,260 >>   Batch size = 8\n",
      "100% 7/7 [00:18<00:00,  2.65s/it]\n",
      "02/09/2021 11:17:45 - INFO - __main__ -   ***** Eval results *****\n",
      "02/09/2021 11:17:45 - INFO - __main__ -     perplexity = 11.975138035600446\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbgV2Fe6hJ6n"
   },
   "source": [
    "### Fine tuning with **7 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q4W6zZ8thXS1",
    "outputId": "801fc9a2-cbc8-4be6-e96a-1c07eb620334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 11:20:44.331601: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/09/2021 11:20:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/09/2021 11:20:45 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=7.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb09_11-20-45_aa1c1fd0c52e, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 11:20:45,883 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 11:20:45,884 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 11:20:45,985 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 11:20:45,986 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 11:20:46,054 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 11:20:46,054 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 11:20:46,055 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-09 11:20:46,134 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-09 11:20:58,341 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-09 11:20:58,341 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-09 11:20:59,484 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-87250f9a92327413.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-738404bd7c994409.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-432212c93cdcf829.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-09d4cdd226cb0aca.arrow\n",
      "[INFO|trainer.py:432] 2021-02-09 11:21:05,495 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-09 11:21:05,495 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-09 11:21:05,496 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:502] 2021-02-09 11:21:05,496 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:502] 2021-02-09 11:21:05,501 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-09 11:21:05,501 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-09 11:21:05,501 >>   Num examples = 472\n",
      "[INFO|trainer.py:839] 2021-02-09 11:21:05,501 >>   Num Epochs = 7\n",
      "[INFO|trainer.py:840] 2021-02-09 11:21:05,501 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-09 11:21:05,501 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-09 11:21:05,502 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-09 11:21:05,502 >>   Total optimization steps = 3304\n",
      "[WARNING|training_args.py:502] 2021-02-09 11:21:06,475 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/3304 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 14% 472/3304 [05:17<32:04,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 11:26:24,200 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:26:24,200 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:26:24,201 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 43% 3/7 [00:02<00:03,  1.03it/s]\u001b[A\n",
      " 57% 4/7 [00:05<00:04,  1.54s/it]\u001b[A\n",
      " 71% 5/7 [00:08<00:03,  1.94s/it]\u001b[A\n",
      " 86% 6/7 [00:11<00:02,  2.22s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.320230007171631, 'eval_runtime': 18.8737, 'eval_samples_per_second': 2.755, 'epoch': 1.0}\n",
      " 14% 472/3304 [05:36<32:04,  1.47it/s]\n",
      "100% 7/7 [00:15<00:00,  2.42s/it]\u001b[A\n",
      "{'loss': 2.4222, 'learning_rate': 5e-05, 'epoch': 1.06}\n",
      " 29% 944/3304 [10:56<26:38,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 11:32:03,318 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:32:03,318 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:32:03,318 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.2977471351623535, 'eval_runtime': 18.603, 'eval_samples_per_second': 2.795, 'epoch': 2.0}\n",
      " 29% 944/3304 [11:15<26:38,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 2.1414, 'learning_rate': 5e-05, 'epoch': 2.12}\n",
      " 43% 1416/3304 [16:35<21:15,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 11:37:42,047 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:37:42,047 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:37:42,047 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.325777053833008, 'eval_runtime': 18.6462, 'eval_samples_per_second': 2.789, 'epoch': 3.0}\n",
      " 43% 1416/3304 [16:54<21:15,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.63s/it]\u001b[A\n",
      "{'loss': 1.9732, 'learning_rate': 5e-05, 'epoch': 3.18}\n",
      " 57% 1888/3304 [22:13<15:57,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 11:43:20,317 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:43:20,317 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:43:20,317 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.3956875801086426, 'eval_runtime': 18.6616, 'eval_samples_per_second': 2.786, 'epoch': 4.0}\n",
      " 57% 1888/3304 [22:32<15:57,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.63s/it]\u001b[A\n",
      "{'loss': 1.8145, 'learning_rate': 5e-05, 'epoch': 4.24}\n",
      " 71% 2360/3304 [27:52<10:41,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 11:48:58,795 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:48:58,795 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:48:58,795 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.45s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.88s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.18s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.39s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.54s/it]\u001b[A\n",
      "100% 7/7 [00:17<00:00,  2.64s/it]\u001b[A\n",
      "{'eval_loss': 2.482832670211792, 'eval_runtime': 18.7404, 'eval_samples_per_second': 2.775, 'epoch': 5.0}\n",
      "\n",
      " 71% 2360/3304 [28:11<10:41,  1.47it/s]\n",
      "{'loss': 1.6563, 'learning_rate': 5e-05, 'epoch': 5.3}\n",
      " 86% 2832/3304 [33:30<05:19,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 11:54:37,472 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 11:54:37,472 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 11:54:37,472 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "100% 7/7 [00:17<00:00,  2.63s/it]\u001b[A\n",
      "{'eval_loss': 2.630836009979248, 'eval_runtime': 18.6593, 'eval_samples_per_second': 2.787, 'epoch': 6.0}\n",
      "\n",
      " 86% 2832/3304 [33:49<05:19,  1.48it/s]\n",
      "{'loss': 1.4961, 'learning_rate': 5e-05, 'epoch': 6.36}\n",
      "100% 3304/3304 [39:09<00:00,  1.46it/s][INFO|trainer.py:1614] 2021-02-09 12:00:16,274 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:00:16,274 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:00:16,274 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.53s/it]\u001b[A\n",
      "100% 7/7 [00:17<00:00,  2.63s/it]\u001b[A\n",
      "{'eval_loss': 2.7215940952301025, 'eval_runtime': 18.6974, 'eval_samples_per_second': 2.781, 'epoch': 7.0}\n",
      "\n",
      "100% 3304/3304 [39:28<00:00,  1.46it/s]\n",
      "                                 \u001b[A[INFO|trainer.py:1011] 2021-02-09 12:00:34,972 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2369.4709, 'train_samples_per_second': 1.394, 'epoch': 7.0}\n",
      "100% 3304/3304 [39:28<00:00,  1.39it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-09 12:00:34,974 >> Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-09 12:00:34,976 >> Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-09 12:00:41,984 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/09/2021 12:00:42 - INFO - __main__ -   ***** Train results *****\n",
      "02/09/2021 12:00:42 - INFO - __main__ -     epoch = 7.0\n",
      "02/09/2021 12:00:42 - INFO - __main__ -     train_runtime = 2369.4709\n",
      "02/09/2021 12:00:42 - INFO - __main__ -     train_samples_per_second = 1.394\n",
      "02/09/2021 12:00:42 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-09 12:00:42,067 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:00:42,067 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:00:42,067 >>   Batch size = 8\n",
      "100% 7/7 [00:18<00:00,  2.64s/it]\n",
      "02/09/2021 12:01:00 - INFO - __main__ -   ***** Eval results *****\n",
      "02/09/2021 12:01:00 - INFO - __main__ -     perplexity = 15.20454042214789\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 7 \\\n",
    "--fp16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyQIU3eXqzha"
   },
   "source": [
    "### Fine tuning with **Learning rate = 4e-5** & number of **epochs = 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dIzrmprOq9mJ",
    "outputId": "93244162-e77d-407a-f8c9-b0163d852606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 12:02:47.056848: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/09/2021 12:02:48 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/09/2021 12:02:48 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=4e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb09_12-02-48_aa1c1fd0c52e, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 12:02:48,497 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 12:02:48,498 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 12:02:48,513 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 12:02:48,514 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 12:02:48,585 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 12:02:48,585 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 12:02:48,585 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-09 12:02:48,664 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-09 12:03:00,787 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-09 12:03:00,787 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-09 12:03:01,848 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-87250f9a92327413.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-738404bd7c994409.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-432212c93cdcf829.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-09d4cdd226cb0aca.arrow\n",
      "[INFO|trainer.py:432] 2021-02-09 12:03:07,698 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-09 12:03:07,699 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-09 12:03:07,699 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:502] 2021-02-09 12:03:07,700 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:502] 2021-02-09 12:03:07,705 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-09 12:03:07,705 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-09 12:03:07,705 >>   Num examples = 472\n",
      "[INFO|trainer.py:839] 2021-02-09 12:03:07,705 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:840] 2021-02-09 12:03:07,705 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-09 12:03:07,705 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-09 12:03:07,705 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-09 12:03:07,705 >>   Total optimization steps = 2360\n",
      "[WARNING|training_args.py:502] 2021-02-09 12:03:08,741 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/2360 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 20% 472/2360 [05:18<21:19,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 12:08:26,953 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:08:26,953 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:08:26,954 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 43% 3/7 [00:02<00:03,  1.03it/s]\u001b[A\n",
      " 57% 4/7 [00:05<00:04,  1.54s/it]\u001b[A\n",
      " 71% 5/7 [00:08<00:03,  1.94s/it]\u001b[A\n",
      " 86% 6/7 [00:11<00:02,  2.22s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.321195363998413, 'eval_runtime': 18.8569, 'eval_samples_per_second': 2.758, 'epoch': 1.0}\n",
      " 20% 472/2360 [05:37<21:19,  1.48it/s]\n",
      "100% 7/7 [00:15<00:00,  2.42s/it]\u001b[A\n",
      "{'loss': 2.4302, 'learning_rate': 4e-05, 'epoch': 1.06}\n",
      " 40% 944/2360 [10:57<16:03,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:14:05,905 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:14:05,905 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:14:05,905 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.45s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.88s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.18s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.39s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.54s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.294205665588379, 'eval_runtime': 18.7634, 'eval_samples_per_second': 2.771, 'epoch': 2.0}\n",
      " 40% 944/2360 [11:15<16:03,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.64s/it]\u001b[A\n",
      "{'loss': 2.1707, 'learning_rate': 4e-05, 'epoch': 2.12}\n",
      " 60% 1416/2360 [16:36<10:40,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:19:45,029 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:19:45,029 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:19:45,029 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.310518741607666, 'eval_runtime': 18.6455, 'eval_samples_per_second': 2.789, 'epoch': 3.0}\n",
      " 60% 1416/2360 [16:54<10:40,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.63s/it]\u001b[A\n",
      "{'loss': 2.0241, 'learning_rate': 4e-05, 'epoch': 3.18}\n",
      " 80% 1888/2360 [22:15<05:21,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:25:24,220 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:25:24,220 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:25:24,220 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.88s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.18s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.39s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.53s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.3612704277038574, 'eval_runtime': 18.7419, 'eval_samples_per_second': 2.775, 'epoch': 4.0}\n",
      " 80% 1888/2360 [22:34<05:21,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.64s/it]\u001b[A\n",
      "{'loss': 1.8861, 'learning_rate': 4e-05, 'epoch': 4.24}\n",
      "100% 2360/2360 [27:54<00:00,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:31:03,244 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:31:03,244 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:31:03,244 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.53s/it]\u001b[A\n",
      "100% 7/7 [00:17<00:00,  2.64s/it]\u001b[A\n",
      "{'eval_loss': 2.4297025203704834, 'eval_runtime': 18.7282, 'eval_samples_per_second': 2.777, 'epoch': 5.0}\n",
      "\n",
      "100% 2360/2360 [28:13<00:00,  1.47it/s]\n",
      "                                 \u001b[A[INFO|trainer.py:1011] 2021-02-09 12:31:21,973 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1694.268, 'train_samples_per_second': 1.393, 'epoch': 5.0}\n",
      "100% 2360/2360 [28:13<00:00,  1.39it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-09 12:31:21,975 >> Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-09 12:31:21,977 >> Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-09 12:31:28,775 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/09/2021 12:31:28 - INFO - __main__ -   ***** Train results *****\n",
      "02/09/2021 12:31:28 - INFO - __main__ -     epoch = 5.0\n",
      "02/09/2021 12:31:28 - INFO - __main__ -     train_runtime = 1694.268\n",
      "02/09/2021 12:31:28 - INFO - __main__ -     train_samples_per_second = 1.393\n",
      "02/09/2021 12:31:28 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-09 12:31:28,852 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:31:28,852 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:31:28,852 >>   Batch size = 8\n",
      "100% 7/7 [00:18<00:00,  2.66s/it]\n",
      "02/09/2021 12:31:47 - INFO - __main__ -   ***** Eval results *****\n",
      "02/09/2021 12:31:47 - INFO - __main__ -     perplexity = 11.355503546515935\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--learning_rate 4e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9asJ-TnwZN3"
   },
   "source": [
    "#### Fine tuning with **Learning rate = 3e-5** & number of **epochs = 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "twlSvF4sweOF",
    "outputId": "42581888-fe40-407e-acf1-2893618856bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 12:31:54.267605: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/09/2021 12:31:56 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/09/2021 12:31:56 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb09_12-31-56_aa1c1fd0c52e, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 12:31:56,465 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 12:31:56,465 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 12:31:56,480 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 12:31:56,481 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 12:31:56,549 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 12:31:56,549 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 12:31:56,549 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-09 12:31:56,635 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-09 12:32:08,599 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-09 12:32:08,599 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-09 12:32:09,652 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-87250f9a92327413.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-738404bd7c994409.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-432212c93cdcf829.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-09d4cdd226cb0aca.arrow\n",
      "[INFO|trainer.py:432] 2021-02-09 12:32:15,468 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-09 12:32:15,468 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-09 12:32:15,469 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:502] 2021-02-09 12:32:15,469 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:502] 2021-02-09 12:32:15,474 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-09 12:32:15,474 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-09 12:32:15,474 >>   Num examples = 472\n",
      "[INFO|trainer.py:839] 2021-02-09 12:32:15,474 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:840] 2021-02-09 12:32:15,474 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-09 12:32:15,474 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-09 12:32:15,475 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-09 12:32:15,475 >>   Total optimization steps = 2360\n",
      "[WARNING|training_args.py:502] 2021-02-09 12:32:16,486 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/2360 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 20% 472/2360 [05:20<21:26,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:37:37,031 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:37:37,031 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:37:37,032 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 43% 3/7 [00:02<00:03,  1.03it/s]\u001b[A\n",
      " 57% 4/7 [00:05<00:04,  1.54s/it]\u001b[A\n",
      " 71% 5/7 [00:08<00:03,  1.94s/it]\u001b[A\n",
      " 86% 6/7 [00:11<00:02,  2.21s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.3234000205993652, 'eval_runtime': 18.8103, 'eval_samples_per_second': 2.764, 'epoch': 1.0}\n",
      " 20% 472/2360 [05:39<21:26,  1.47it/s]\n",
      "100% 7/7 [00:15<00:00,  2.41s/it]\u001b[A\n",
      "{'loss': 2.4431, 'learning_rate': 3e-05, 'epoch': 1.06}\n",
      " 40% 944/2360 [11:00<16:01,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:43:16,887 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:43:16,887 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:43:16,887 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.45s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.18s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.39s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.54s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.2963125705718994, 'eval_runtime': 18.7416, 'eval_samples_per_second': 2.775, 'epoch': 2.0}\n",
      " 40% 944/2360 [11:19<16:01,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.64s/it]\u001b[A\n",
      "{'loss': 2.2079, 'learning_rate': 3e-05, 'epoch': 2.12}\n",
      " 60% 1416/2360 [16:39<10:40,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:48:56,222 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:48:56,222 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:48:56,222 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.39s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.53s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.296740770339966, 'eval_runtime': 18.7155, 'eval_samples_per_second': 2.778, 'epoch': 3.0}\n",
      " 60% 1416/2360 [16:58<10:40,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.63s/it]\u001b[A\n",
      "{'loss': 2.0913, 'learning_rate': 3e-05, 'epoch': 3.18}\n",
      " 80% 1888/2360 [22:19<05:20,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 12:54:35,871 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 12:54:35,871 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 12:54:35,871 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.53s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.3258161544799805, 'eval_runtime': 18.7113, 'eval_samples_per_second': 2.779, 'epoch': 4.0}\n",
      " 80% 1888/2360 [22:38<05:20,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.64s/it]\u001b[A\n",
      "{'loss': 1.9787, 'learning_rate': 3e-05, 'epoch': 4.24}\n",
      "100% 2360/2360 [27:58<00:00,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 13:00:15,329 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:00:15,329 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:00:15,329 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.45s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.88s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.18s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.39s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.54s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.372678279876709, 'eval_runtime': 18.7675, 'eval_samples_per_second': 2.771, 'epoch': 5.0}\n",
      "100% 2360/2360 [28:17<00:00,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.64s/it]\u001b[A\n",
      "                                 \u001b[A[INFO|trainer.py:1011] 2021-02-09 13:00:34,097 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1698.6229, 'train_samples_per_second': 1.389, 'epoch': 5.0}\n",
      "100% 2360/2360 [28:17<00:00,  1.39it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-09 13:00:34,099 >> Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-09 13:00:34,101 >> Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-09 13:00:41,439 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/09/2021 13:00:41 - INFO - __main__ -   ***** Train results *****\n",
      "02/09/2021 13:00:41 - INFO - __main__ -     epoch = 5.0\n",
      "02/09/2021 13:00:41 - INFO - __main__ -     train_runtime = 1698.6229\n",
      "02/09/2021 13:00:41 - INFO - __main__ -     train_samples_per_second = 1.389\n",
      "02/09/2021 13:00:41 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-09 13:00:41,523 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:00:41,523 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:00:41,523 >>   Batch size = 8\n",
      "100% 7/7 [00:18<00:00,  2.65s/it]\n",
      "02/09/2021 13:01:00 - INFO - __main__ -   ***** Eval results *****\n",
      "02/09/2021 13:01:00 - INFO - __main__ -     perplexity = 10.726081296051238\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--learning_rate 3e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBIuyEcmwiZF"
   },
   "source": [
    "#### Fine tuning with **Learning rate = 2e-5** & number of **epochs = 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r413bD3Jwmdg",
    "outputId": "7ce43437-2c1d-4a01-ee60-26c07d000ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-09 13:01:03.892428: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/09/2021 13:01:05 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/09/2021 13:01:05 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb09_13-01-05_aa1c1fd0c52e, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 13:01:05,510 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 13:01:05,510 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-09 13:01:05,526 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-09 13:01:05,527 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 13:01:05,600 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 13:01:05,600 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-09 13:01:05,600 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-09 13:01:05,681 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-09 13:01:17,776 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-09 13:01:17,776 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-09 13:01:18,824 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484138 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-87250f9a92327413.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-738404bd7c994409.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-432212c93cdcf829.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-60751f6236c8b2cc/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-09d4cdd226cb0aca.arrow\n",
      "[INFO|trainer.py:432] 2021-02-09 13:01:24,695 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-09 13:01:24,696 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-09 13:01:24,696 >> Using amp fp16 backend\n",
      "[WARNING|training_args.py:502] 2021-02-09 13:01:24,697 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:502] 2021-02-09 13:01:24,702 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-09 13:01:24,702 >> ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-09 13:01:24,702 >>   Num examples = 472\n",
      "[INFO|trainer.py:839] 2021-02-09 13:01:24,702 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:840] 2021-02-09 13:01:24,702 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-09 13:01:24,702 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-09 13:01:24,702 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-09 13:01:24,702 >>   Total optimization steps = 2360\n",
      "[WARNING|training_args.py:502] 2021-02-09 13:01:25,774 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/2360 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      " 20% 472/2360 [05:20<21:22,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 13:06:46,086 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:06:46,086 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:06:46,087 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 43% 3/7 [00:02<00:03,  1.04it/s]\u001b[A\n",
      " 57% 4/7 [00:05<00:04,  1.53s/it]\u001b[A\n",
      " 71% 5/7 [00:08<00:03,  1.93s/it]\u001b[A\n",
      " 86% 6/7 [00:11<00:02,  2.21s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.3357057571411133, 'eval_runtime': 18.7961, 'eval_samples_per_second': 2.767, 'epoch': 1.0}\n",
      " 20% 472/2360 [05:39<21:22,  1.47it/s]\n",
      "100% 7/7 [00:15<00:00,  2.41s/it]\u001b[A\n",
      "{'loss': 2.47, 'learning_rate': 2e-05, 'epoch': 1.06}\n",
      " 40% 944/2360 [10:59<15:58,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 13:12:25,525 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:12:25,525 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:12:25,525 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.15s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.51s/it]\u001b[A\n",
      "                                      \n",
      "\u001b[A{'eval_loss': 2.3053476810455322, 'eval_runtime': 18.591, 'eval_samples_per_second': 2.797, 'epoch': 2.0}\n",
      " 40% 944/2360 [11:18<15:58,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 2.2541, 'learning_rate': 2e-05, 'epoch': 2.12}\n",
      " 60% 1416/2360 [16:38<10:37,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 13:18:04,333 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:18:04,333 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:18:04,333 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.52s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.294973134994507, 'eval_runtime': 18.6333, 'eval_samples_per_second': 2.791, 'epoch': 3.0}\n",
      " 60% 1416/2360 [16:57<10:37,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 2.1684, 'learning_rate': 2e-05, 'epoch': 3.18}\n",
      " 80% 1888/2360 [22:17<05:19,  1.48it/s][INFO|trainer.py:1614] 2021-02-09 13:23:42,827 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:23:42,827 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:23:42,827 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.43s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.86s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.16s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.37s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.51s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.302309274673462, 'eval_runtime': 18.5709, 'eval_samples_per_second': 2.8, 'epoch': 4.0}\n",
      " 80% 1888/2360 [22:35<05:19,  1.48it/s]\n",
      "100% 7/7 [00:18<00:00,  2.62s/it]\u001b[A\n",
      "{'loss': 2.0903, 'learning_rate': 2e-05, 'epoch': 4.24}\n",
      "100% 2360/2360 [27:55<00:00,  1.47it/s][INFO|trainer.py:1614] 2021-02-09 13:29:21,243 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:29:21,244 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:29:21,244 >>   Batch size = 8\n",
      "\n",
      "  0% 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 29% 2/7 [00:02<00:07,  1.44s/it]\u001b[A\n",
      " 43% 3/7 [00:05<00:07,  1.87s/it]\u001b[A\n",
      " 57% 4/7 [00:08<00:06,  2.17s/it]\u001b[A\n",
      " 71% 5/7 [00:11<00:04,  2.38s/it]\u001b[A\n",
      " 86% 6/7 [00:14<00:02,  2.53s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.3186821937561035, 'eval_runtime': 18.6754, 'eval_samples_per_second': 2.784, 'epoch': 5.0}\n",
      "100% 2360/2360 [28:14<00:00,  1.47it/s]\n",
      "100% 7/7 [00:18<00:00,  2.63s/it]\u001b[A\n",
      "                                 \u001b[A[INFO|trainer.py:1011] 2021-02-09 13:29:39,920 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1695.2175, 'train_samples_per_second': 1.392, 'epoch': 5.0}\n",
      "100% 2360/2360 [28:14<00:00,  1.39it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-09 13:29:39,922 >> Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-09 13:29:39,924 >> Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-09 13:29:47,525 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/09/2021 13:29:47 - INFO - __main__ -   ***** Train results *****\n",
      "02/09/2021 13:29:47 - INFO - __main__ -     epoch = 5.0\n",
      "02/09/2021 13:29:47 - INFO - __main__ -     train_runtime = 1695.2175\n",
      "02/09/2021 13:29:47 - INFO - __main__ -     train_samples_per_second = 1.392\n",
      "02/09/2021 13:29:47 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-09 13:29:47,602 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-09 13:29:47,602 >>   Num examples = 52\n",
      "[INFO|trainer.py:1616] 2021-02-09 13:29:47,602 >>   Batch size = 8\n",
      "100% 7/7 [00:18<00:00,  2.65s/it]\n",
      "02/09/2021 13:30:06 - INFO - __main__ -   ***** Eval results *****\n",
      "02/09/2021 13:30:06 - INFO - __main__ -     perplexity = 10.162273570662657\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--learning_rate 2e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218,
     "referenced_widgets": [
      "07ac858352fc4e07a60eaa84310cc5a1",
      "4d112cf47cc9482fbbf8d703c10559d6",
      "dac659c527164f37b209e7aef340fb5b",
      "c45243609ad244f3aeceb11d3e213bbc",
      "56a388a1507845bc9537b3e60d9e7477",
      "f49cb30152974fb8a741c0c057ec365f",
      "684f3b3ad6284b80ac719c34e566a4b1",
      "aaa17c418006450cb2596e8c263817c8",
      "d43a635fa08a426887e5b112f73aebeb",
      "7b9dff47948a476183fc396e06b97b96",
      "e06d435e204d46d2a900142f78a9772a",
      "e958882ffbf4475997bff24cc1ed7229",
      "be160c60e7794714ad2bdd1a08c5b1a5",
      "ed44fe544eab46778cbe6e0d02acc73b",
      "e40a9cf9e1af47f29b1c97ec750b0469",
      "bf290e537239425191264f8bbfb85f80"
     ]
    },
    "id": "iMDmm7TEVpb4",
    "outputId": "dcc1534d-16b0-4e28-efab-47786ba83a31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFGPT2LMHeadModel: ['transformer.h.22.attn.masked_bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.17.attn.masked_bias', 'lm_head.weight', 'transformer.h.14.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.23.attn.masked_bias']\n",
      "- This IS expected if you are initializing TFGPT2LMHeadModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFGPT2LMHeadModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ac858352fc4e07a60eaa84310cc5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43a635fa08a426887e5b112f73aebeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"Output dir\", from_pt = True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEpvmAoWB2qh"
   },
   "source": [
    "## Part 4: Shakespeare Text Generation using GPT-2 <a name=\"TextGeneration\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGXQxXhqV2Vz",
    "outputId": "dfa225da-a79a-4292-8587-85a1865064fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=int32, numpy=\n",
       "array([ 1544,   326,   481,  1577,   922,  2456,   284, 17903,   481],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the initial text to start the process of text generation\n",
    "input_ids = tokenizer.encode(\"He that will give good words to thee will\", return_tensors = 'tf')\n",
    "\n",
    "# print the tensor ids\n",
    "input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCyei38MWUlr",
    "outputId": "78c5ead8-63a9-4890-c94a-f069db77d165"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "generated_text_samples = model.generate(\n",
    "    input_ids, \n",
    "    max_length = 128,  \n",
    "    num_return_sequences = 5,\n",
    "    no_repeat_ngram_size = 2,\n",
    "    repetition_penalty = 1.5,\n",
    "    top_p = 0.92,\n",
    "    temperature = 0.85,\n",
    "    do_sample = True,\n",
    "    top_k = 125,\n",
    "    early_stopping = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ChFrv99WiiC",
    "outputId": "c63036c1-e65e-4ad8-c37a-1ac26f3d7653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: He that will give good words to thee will also deliver thy spirit into the hands of God. - Psalm 83:20.\n",
      "There is a reason for this prophecy, however; it points to Christ's return and His coming again on Earth upon his own behalf (see Hebrews 13). However how does he accomplish these goals by making himself available at once? And then why did Jesus bring back all those things which have been stolen from Him as He had already done so in Heaven before leaving us here today just years ago? On one side are various Christian saints who would not tolerate their former status with our modern world but still remain faithful\n",
      "\n",
      "1: He that will give good words to thee will be more profitable for me than any king. And let him who has been made a great man say unto his disciples, O men! I have laid before you all the kingdom of Heaven; and thy place is this: ye shall not go down from it until thou canst see clearly what my wrath hath caused iniquity by those things which are done among us.\"\n",
      "17 Then Jesus said these mighty sentences on them saying with trembling heart : \"My Lord!\" They cried out continually against Him also crying many times loudly ; 17 but he answered no answer whatsoever till they came near Mary's room\n",
      "\n",
      "2: He that will give good words to thee will send me off with great effect. Yea, if thou wilt be merciful unto him who is in my hand when I return.\"\n",
      "Dionysus was sitting on the right and Cynosma (crown) would have been kneeling upon his breast as before while he spoke; but she had not yet turned her eyes from them or bowed down at any time for fear lest it might hurt Diana's honor too much: \"I am coming hither alone,\" said they all together of course without speaking a word about how well-disciplined Democritius should receive their commands\n",
      "\n",
      "3: He that will give good words to thee will make thy father a god. Then shall I turn my back on you, and see if thou wilt come into the way of any man.\"\n",
      "\"But what do ye think?\" said he with great astonishment, \"what does it mean? for Christ hath made our fathers gods in this world; wherefore we have not been given up yet by them without justification or pardon until He comes again after us upon earth. And now how are they coming once more before me when there is no God among all men? Why then hast Thou spoken so much unto these children whom We had foretold\n",
      "\n",
      "4: He that will give good words to thee will bring me an inheritance from her. But she gave away the father's riches unto him; and thou shalt also see how it is done: for we are not poor but rich people in this world, nor were they before made blind when ye dwelt with them.\"\n",
      "In verse 36 of Mark 9 I'm told by one male friend who believes he was \"the son\" at his grandmothers wedding (his name) where Mary lived until 1978 because a new bride had already bought himself more than $40k worth—her grandfather owned about four acres near Bethlehem City.... The only problem? It\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print output for each sequence generated above\n",
    "for i, token in enumerate(generated_text_samples):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(token, skip_special_tokens = True)))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esan38nSBrNc"
   },
   "source": [
    "## Part 5: Bill Sum Data set preparation<a name=\"BillSum\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_LxGI3EzBqS-",
    "outputId": "2ffec9fe-9d59-446e-fbc7-c507a23c1a30"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bill_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "      <th>text_len</th>\n",
       "      <th>sum_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>107_hr2256</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be cited as the ``Border Hospital Survival and Illegal \\nImmigrant Care Act''.\\n\\nSEC. 2. FINDINGS.\\n\\n    The Congress finds as follows:\\n            (1) Immigration is a Federal responsibility.\\n            (2) The Immigration and Naturalization Service does not \\n        take into custody all aliens who are unlawfully present in the \\n        United States.\\n            (3) Section 1867 of the Social Security Act (42 U.S.C. \\n        1395dd) and State laws require that, if any individual (whether \\n        or not lawfully present in the United States) comes to a \\n        hospital and the hospital determines that the individual has an \\n        emergency medical condition, the hospital must provide either, \\n        within the staff and facilities available at the hospital, for \\n        such further medical examination and such treatment as may be \\n        required to stabilize the medical condition, or, if \\n        appropriate, for transfer of the individual to another medical \\n        facility.\\n            (4) The Southwest border region is ill-equipped to absorb \\n        the expense of providing health care to undocumented aliens \\n        because it ranks last in the country in terms of per capita \\n        income.\\n            (5) The Southwest border region has been designated as a \\n        health professional shortage area under section 332 of the \\n        Public Health Service Act (42 U.S.C. 254e).\\n            (6) The unreimbursed costs associated with caring for \\n        undocumented aliens are severely threatening the financial \\n        stability of health care providers in Arizona.\\n\\nSEC. 3. REIMBURSEMENT TO HEALTH CARE PROVIDERS FOR EMERGENCY MEDICAL \\n              CARE RENDERED TO CERTAIN ALIENS.\\n\\n    Section 322 of the Public Health Service Act (42 U.S.C. 249) is \\namended by adding at the end the following:\\n    ``(d)(1) The Secretary shall establish and implement a 5-year pilot \\nprogram under which funds made available under paragraph (6) are used \\nto reimburse providers for items and services described in section \\n411(b)(1) of the Personal Responsibility and Work Opportunity \\nReconciliation Act of 1996 (8 U.S.C. 1621(b)(1)) provided in Arizona to \\naliens described in paragraph (3), and to reimburse suppliers of \\nemergency ambulance services furnished to such aliens for which the \\ntransportation originates in Arizona (where the use of other methods of \\ntransportation is contraindicated by the alien's condition), if payment \\nmay not be made to reimburse the provider or supplier under any Federal \\nprogram or law other than this subsection (such as title XIX of the \\nSocial Security Act), any State or local program or law, any group or \\nindividual health plan, or any insurance policy.\\n    ``(2) As part of the pilot program, in a case in which an alien \\ndescribed in paragraph (3) arrived at a hospital in Arizona and the \\nhospital provided for such medical examination and treatment of the \\nalien as the hospital determined was required to stabilize an emergency \\nmedical condition (within the meaning of section 1867(e)(1) of the \\nSocial Security Act (42 U.S.C. 1395dd(e)(1))), the Secretary shall use \\nfunds made available under paragraph (6) to reimburse the hospital for \\nany transportation costs paid by the hospital to return the alien to \\nthe United States border, if--\\n            ``(A) the hospital requested the Attorney General to take \\n        the alien into custody after such stabilization;\\n            ``(B) such request was denied within 24 hours after its \\n        receipt, or the Attorney General gave no response to it within \\n        such period; and\\n            ``(C) the hospital determined that discharging the alien \\n        without providing for such transportation might pose a threat \\n        to the health or safety of the alien (or, with respect to a \\n        pregnant alien, the health or safety of the alien or her unborn \\n        child).\\n    ``(3) An alien is described in this paragraph if the alien--\\n            ``(A) is not lawfully present in the United States and not \\n        detained by any Federal, State, or local law enforcement \\n        authority; or\\n            ``(B) is paroled into the United States under section \\n        212(d)(5) of the Immigration and Nationality Act (8 U.S.C. \\n        1182(d)(5)) for less than one year in order to receive \\n        treatment for an emergency medical condition.\\n    ``(4) During the period in which the pilot program is operating, \\nthe Secretary shall submit annual reports to the Congress on its \\noperation. Each report shall contain at least the following \\ninformation:\\n            ``(A) The number of aliens to whom assistance was rendered \\n        for which payment was made under this subsection during the \\n        previous year.\\n            ``(B) The nationality of such aliens.\\n            ``(C) The average cost per alien of such assistance.\\n            ``(D) The total annual amount paid to each provider or \\n        supplier of assistance.\\n            ``(E) The feasibility and estimated cost of expanding the \\n        pilot program to items and services provided anywhere in the \\n        Southwest border region of the United States.\\n    ``(5) Nothing in this subsection shall be construed to authorize \\nany reduction in the funds payable to any person under any Federal \\nprogram or law other than this subsection (such as title XIX of the \\nSocial Security Act), any State or local program or law, any group or \\nindividual health plan, or any insurance policy.\\n    ``(6) To the extent provided in appropriations Acts, from amounts \\nmade available to the Immigration and Naturalization Service for \\nenforcement and border affairs for each of the 5 fiscal years following \\nthe fiscal year in which the Border Hospital Survival and Illegal \\nImmigrant Care Act is enacted, the Attorney General may transfer to the \\nHealth Resources and Services Administration of the Department of \\nHealth and Human Services such amounts as may be necessary to carry out \\nthis subsection, not to exceed $50,000,000 for each such year.''.</td>\n",
       "      <td>Border Hospital Survival and Illegal Immigrant Care Act - Amends the Public Health Service Act to direct the Secretary of Health and Human Services to establish a five-year pilot program of health care provider reimbursement for the costs associated with providing emergency medical and ambulance services in Arizona to: (1) illegal aliens who are not detained by any Federal, State, or local law enforcement authority. Or (2) aliens paroled into the United States for less than one year to receive emergency medical treatment.</td>\n",
       "      <td>To amend the Public Health Service Act to establish a 5-year pilot program under which health care providers are reimbursed by the Secretary of Health and Human Services for the costs associated with providing emergency medical care to aliens who are not lawfully present in the United States and are not detained by any law enforcement authority, and for other purposes.</td>\n",
       "      <td>6100</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111_hr4710</td>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be cited as the ``Farm to School Improvements Act of \\n2010''.\\n\\nSEC. 2. FARM TO SCHOOL PROGRAM.\\n\\n    (a) Amendment.--The Richard B. Russell National School Lunch Act \\n(42 U.S.C. 1751 et seq.) is amended by inserting after section 19, the \\nfollowing:\\n\\n``SEC. 19A. FARM TO SCHOOL PROGRAM.\\n\\n    ``(a) In General.--The Secretary shall provide assistance, through \\ncompetitive matching grants and technical assistance, to eligible \\nentities for farm to school programs that--\\n            ``(1) improve access to local foods in schools and \\n        institutions participating in programs under this Act and \\n        section 4 of the Child Nutrition Act of 1966 (42 U.S.C. 1773) \\n        through farm to school activities, including the purchase of \\n        local food, establishment of effective relationships between \\n        school and institutional food service providers, distributors, \\n        and producers or groups of producers, school gardens, \\n        appropriate equipment, and the provision of training and \\n        education; and\\n            ``(2) are designed to--\\n                    ``(A) improve the nutritional health and well being \\n                of children;\\n                    ``(B) procure healthy local foods from small and \\n                medium-sized farms for meals at eligible schools and \\n                institutions;\\n                    ``(C) support experiential nutrition education \\n                activities and curriculum planning that incorporates \\n                the participation of school children in farm and \\n                garden-based agricultural education activities;\\n                    ``(D) develop a sustained commitment to farm to \\n                school programs in the community by linking schools and \\n                institutions, State and local agencies including Indian \\n                Tribal Organizations, institutions of higher education, \\n                agricultural producers, parents, community garden \\n                groups and other community stakeholders; and\\n                    ``(E) increase farm income by facilitating farmers' \\n                access to institutional markets including schools.\\n    ``(b) Eligible Entity.--For purposes of this section, the term \\n`eligible entity' means--\\n            ``(1) a school;\\n            ``(2) nonprofit organization; or\\n            ``(3) other entity that the Secretary determines offers a \\n        unique ability to provide services or farm-to-school programs.\\n    ``(c) Grants.--\\n            ``(1) Types of grants.--A grant awarded under this section \\n        may include--\\n                    ``(A) an implementation grant to support the cost \\n                of implementing a farm to school program;\\n                    ``(B) a training and technical assistance grant to \\n                support the cost of--\\n                            ``(i) providing the training, operational \\n                        support, information, and access to resources \\n                        necessary to implement a successful farm to \\n                        school program; and\\n                            ``(ii) encouraging collaboration between \\n                        public and private entities; or\\n                    ``(C) a planning grant to support the cost of \\n                conducting research, identifying resources, and \\n                developing partnerships to design a successful and \\n                sustainable farm to school program.\\n            ``(2) Grant amounts.--A grant awarded under this section to \\n        an eligible entity shall not exceed--\\n                    ``(A) in the case of an implementation or training \\n                and technical assistance grant, $100,000; and\\n                    ``(B) in the case of a planning grant, $25,000.\\n            ``(3) Grant duration.--A grant under this section shall be \\n        awarded for a period--\\n                    ``(A) in the case of an implementation or training \\n                and technical assistance grant, not to exceed 2 years; \\n                and\\n                    ``(B) in the case of a planning grant, not to \\n                exceed 1 year.\\n    ``(d) Cost Share.--\\n            ``(1) In general.--The amount of a grant made under this \\n        section shall not exceed 75 percent of the cost of the proposed \\n        grant activities.\\n            ``(2) Non-federal support.--A recipient of a grant under \\n        this section shall be required to provide at least 25 percent \\n        of the cost of the proposed grant activities in the form of \\n        cash or in-kind contributions (including facilities, equipment, \\n        training, or services provided by State and local governments \\n        and private sources).\\n    ``(e) Evaluation.--A recipient of a grant under this section shall \\ncooperate in an evaluation by the Secretary of the programs carried out \\nusing such grant funds.\\n    ``(f) Regional Balance.--In making awards and providing technical \\nassistance under this section, the Secretary shall to the maximum \\nextent practicable, ensure--\\n            ``(1) geographical diversity; and\\n            ``(2) equitable treatment of urban, rural, and tribal \\n        communities.\\n    ``(g) Technical Assistance.--The Secretary shall provide recipients \\nof grants under this section with technical assistance, which shall \\ninclude sharing information, best practices, research, and data on \\nexisting farm to school programs.\\n    ``(h) Proposals.--\\n            ``(1) In general.--An eligible entity desiring to receive a \\n        grant under this section shall submit a proposal to the \\n        Secretary at such time, in such manner, and containing such \\n        information as the Secretary may require.\\n            ``(2) Competitive award selection.--The Secretary shall \\n        form review panels to evaluate proposals submitted under \\n        paragraph (1) based on the criteria described in paragraph (3). \\n        Such review panels shall include--\\n                    ``(A) representatives of schools and eligible \\n                institutions;\\n                    ``(B) registered dietitians;\\n                    ``(C) operators of small and medium-sized farms;\\n                    ``(D) public agencies;\\n                    ``(E) non-governmental and community-based \\n                organizations with expertise in local food systems and \\n                farm to school programs; and\\n                    ``(F) other appropriate parties as determined by \\n                the Secretary.\\n            ``(3) Proposal review criteria.--In making awards under \\n        this section, the Secretary shall evaluate proposals based on \\n        the extent to which the proposed program--\\n                    ``(A) improves the nutritional health and well \\n                being of children;\\n                    ``(B) makes local food products available on the \\n                menu of the school or institution;\\n                    ``(C) benefits local small and medium-sized farms;\\n                    ``(D) incorporates experiential nutrition education \\n                activities and curriculum planning that incorporates \\n                the participation of school children in farm and \\n                garden-based agricultural education activities;\\n                    ``(E) serves schools and eligible institutions with \\n                a high proportion of children who are eligible for free \\n                and reduced price lunches;\\n                    ``(F) demonstrates collaboration between schools or \\n                institutions, non-governmental and community-based \\n                organizations, farmer groups, and other community \\n                partners;\\n                    ``(G) demonstrates the potential for long-term \\n                program sustainability;\\n                    ``(H) includes adequate and participatory \\n                evaluation plans; and\\n                    ``(I) meets such other related criteria as the \\n                Secretary may determine relevant.\\n    ``(i) Funding.--Beginning on October 1, 2010, or of any funds in \\nthe Treasury not otherwise appropriated, the Secretary of the Treasury \\nshall transfer to the Secretary of Agriculture to carry out this \\nsection $10,000,000 each fiscal year, to remain available until \\nexpended.''.\\n    (b) Conforming Change.--Section 18(g) of the Richard B. Russell \\nSchool Lunch Act (42 U.S.C. 1769(g)) is amended--\\n            (1) by striking paragraphs (1) and (2); and\\n            (2) by redesignating paragraphs (3) and (4) as paragraphs \\n        (1) and (2), respectively.</td>\n",
       "      <td>Farm to School Improvements Act of 2010 - Amends the Richard B. Russell National School Lunch Act to direct the Secretary of Agriculture to provide competitive matching grants to schools, nonprofit organizations, and other able entities for farm to school programs that improve the access of school lunch and breakfast program participants to local foods. Provides that each grant may include an implementation grant, training and technical assistance grant, and planning grant. Requires farm to school programs to be designed to: (1) improve the nutritional health and well being of children, (2) procure healthy local foods from small and medium-sized farms. (3) support experiential nutrition education by involving school children in farm and garden-based agricultural education activities. (4) commit public and private community stakeholders to the sustained success of such programs. And (5) increase farmers' income by facilitating their access to institutional markets. Directs the Secretary to provide grant recipients with technical assistance that includes sharing information, best practices, research, and data on existing farm to school programs.</td>\n",
       "      <td>To amend the Richard B. Russell National School Lunch Act to award grants to eligible entities for farm to school programs.</td>\n",
       "      <td>8628</td>\n",
       "      <td>1161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bill_id  ... sum_len\n",
       "0  107_hr2256  ...     527\n",
       "1  111_hr4710  ...    1161\n",
       "\n",
       "[2 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the training data into pandas dataframe\n",
    "data_path = '/content/drive/MyDrive/bill_sum_shakespeare/us_train_data_final_OFFICIAL.jsonl'\n",
    "data = pd.read_json(data_path, lines = True)\n",
    "\n",
    "# display settings\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# display few rows of the dataframe\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyEf-zaSofdL",
    "outputId": "3bd545d4-7e9b-48c1-a76a-f13b5b6fde4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18949, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the data frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjS-kNxtpSZj",
    "outputId": "065fd0f8-c825-4955-c68f-4068ee078e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size:17054\n",
      "Evaluation size: 1895\n"
     ]
    }
   ],
   "source": [
    "# extract the text data\n",
    "text = data['summary'].values.tolist()\n",
    "\n",
    "# split data into train and validation set\n",
    "train, eval = train_test_split(text, train_size = 0.9, random_state = 42)\n",
    "print(\"Training size:\" + str(len(train)))\n",
    "print(\"Evaluation size: \" + str(len(eval)))\n",
    "\n",
    "with open('train_tmp.txt', 'w') as file_handle:\n",
    "  file_handle.write(\"<|endoftext|>\".join(train))\n",
    "\n",
    "with open('eval_tmp.txt', 'w') as file_handle:\n",
    "  file_handle.write(\"<|endoftext|>\".join(eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjDqyrOKtHt3"
   },
   "source": [
    "## Part 6: GPT-2 Model Fine tuning on Bill Sum Dataset<a name=\"FineTuningBillSum\"></a>\n",
    "### Fine tuning with **1 epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5s6-a617tACP",
    "outputId": "d0761fde-0130-4e3a-ccdd-7875b990d2ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-10 01:23:20.838664: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/10/2021 01:23:22 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/10/2021 01:23:22 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb10_01-23-22_db3590ecbfe2, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Downloading: 2.57kB [00:00, 3.04MB/s]       \n",
      "Using custom data configuration default\n",
      "Downloading and preparing dataset text/default-3fae79341c5451f5 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab...\n",
      "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab. Subsequent calls will reuse this data.\n",
      "[INFO|file_utils.py:1302] 2021-02-10 01:23:23,623 >> https://huggingface.co/gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_3pqxoa0\n",
      "02/10/2021 01:23:23 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_3pqxoa0\n",
      "Downloading: 100% 718/718 [00:00<00:00, 644kB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 01:23:23,828 >> storing https://huggingface.co/gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 01:23:23 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|file_utils.py:1309] 2021-02-10 01:23:23,828 >> creating metadata file for /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 01:23:23 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 01:23:23,829 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 01:23:23 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 01:23:23,829 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 01:23:23 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 01:23:24,036 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 01:23:24 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 01:23:24,037 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 01:23:24 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|file_utils.py:1302] 2021-02-10 01:23:24,262 >> https://huggingface.co/gpt2-medium/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpblipol13\n",
      "02/10/2021 01:23:24 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpblipol13\n",
      "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.71MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 01:23:24,862 >> storing https://huggingface.co/gpt2-medium/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 01:23:24 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|file_utils.py:1309] 2021-02-10 01:23:24,862 >> creating metadata file for /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 01:23:24 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|file_utils.py:1302] 2021-02-10 01:23:25,082 >> https://huggingface.co/gpt2-medium/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbk5buzdr\n",
      "02/10/2021 01:23:25 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbk5buzdr\n",
      "Downloading: 100% 456k/456k [00:00<00:00, 1.39MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 01:23:25,627 >> storing https://huggingface.co/gpt2-medium/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 01:23:25 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|file_utils.py:1309] 2021-02-10 01:23:25,627 >> creating metadata file for /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 01:23:25 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|file_utils.py:1302] 2021-02-10 01:23:25,844 >> https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd4jdu9zs\n",
      "02/10/2021 01:23:25 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpd4jdu9zs\n",
      "Downloading: 100% 1.36M/1.36M [00:00<00:00, 3.37MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 01:23:26,472 >> storing https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 01:23:26 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|file_utils.py:1309] 2021-02-10 01:23:26,472 >> creating metadata file for /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 01:23:26 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 01:23:26,473 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 01:23:26 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 01:23:26,473 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 01:23:26 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 01:23:26,473 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 01:23:26 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|file_utils.py:1302] 2021-02-10 01:23:26,738 >> https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj8wdq09a\n",
      "02/10/2021 01:23:26 - INFO - transformers.file_utils -   https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpj8wdq09a\n",
      "Downloading: 100% 1.52G/1.52G [00:18<00:00, 81.9MB/s]\n",
      "[INFO|file_utils.py:1306] 2021-02-10 01:23:45,337 >> storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 01:23:45 - INFO - transformers.file_utils -   storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|file_utils.py:1309] 2021-02-10 01:23:45,337 >> creating metadata file for /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 01:23:45 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 01:23:45,337 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 01:23:45 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 01:23:57,656 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "02/10/2021 01:23:57 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 01:23:57,656 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "02/10/2021 01:23:57 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-10 01:24:10,993 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3612570 > 1024). Running this sequence through the model will result in indexing errors\n",
      "02/10/2021 01:24:10 - WARNING - transformers.tokenization_utils_base -   Token indices sequence length is longer than the specified maximum sequence length for this model (3612570 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100% 1/1 [00:12<00:00, 12.93s/ba]\n",
      "100% 1/1 [00:01<00:00,  1.34s/ba]\n",
      "100% 1/1 [00:02<00:00,  2.73s/ba]\n",
      "100% 1/1 [00:00<00:00,  3.43ba/s]\n",
      "[INFO|trainer.py:432] 2021-02-10 01:24:47,273 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-10 01:24:47,273 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-10 01:24:47,274 >> Using amp fp16 backend\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -   Using amp fp16 backend\n",
      "[WARNING|training_args.py:514] 2021-02-10 01:24:47,274 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 01:24:47 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:514] 2021-02-10 01:24:47,280 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 01:24:47 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-10 01:24:47,280 >> ***** Running training *****\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -   ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 01:24:47,280 >>   Num examples = 3527\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -     Num examples = 3527\n",
      "[INFO|trainer.py:839] 2021-02-10 01:24:47,280 >>   Num Epochs = 1\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -     Num Epochs = 1\n",
      "[INFO|trainer.py:840] 2021-02-10 01:24:47,280 >>   Instantaneous batch size per device = 8\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-10 01:24:47,280 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-10 01:24:47,280 >>   Gradient Accumulation steps = 1\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-10 01:24:47,280 >>   Total optimization steps = 3527\n",
      "02/10/2021 01:24:47 - INFO - transformers.trainer -     Total optimization steps = 3527\n",
      "[WARNING|training_args.py:514] 2021-02-10 01:24:47,291 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 01:24:47 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/3527 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "{'loss': 2.4589, 'learning_rate': 5e-05, 'epoch': 0.14}\n",
      "{'loss': 2.3619, 'learning_rate': 5e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3225, 'learning_rate': 5e-05, 'epoch': 0.43}\n",
      "{'loss': 2.2954, 'learning_rate': 5e-05, 'epoch': 0.57}\n",
      "{'loss': 2.2796, 'learning_rate': 5e-05, 'epoch': 0.71}\n",
      "{'loss': 2.2542, 'learning_rate': 5e-05, 'epoch': 0.85}\n",
      "{'loss': 2.2348, 'learning_rate': 5e-05, 'epoch': 0.99}\n",
      "100% 3527/3527 [39:43<00:00,  1.48it/s][INFO|trainer.py:1614] 2021-02-10 02:04:31,031 >> ***** Running Evaluation *****\n",
      "02/10/2021 02:04:31 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 02:04:31,031 >>   Num examples = 391\n",
      "02/10/2021 02:04:31 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 02:04:31,032 >>   Batch size = 8\n",
      "02/10/2021 02:04:31 - INFO - transformers.trainer -     Batch size = 8\n",
      "\n",
      "  0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 3/49 [00:02<00:44,  1.03it/s]\u001b[A\n",
      "  8% 4/49 [00:05<01:09,  1.55s/it]\u001b[A\n",
      " 10% 5/49 [00:08<01:25,  1.95s/it]\u001b[A\n",
      " 12% 6/49 [00:11<01:36,  2.23s/it]\u001b[A\n",
      " 14% 7/49 [00:14<01:42,  2.43s/it]\u001b[A\n",
      " 16% 8/49 [00:17<01:45,  2.57s/it]\u001b[A\n",
      " 18% 9/49 [00:20<01:46,  2.66s/it]\u001b[A\n",
      " 20% 10/49 [00:23<01:46,  2.73s/it]\u001b[A\n",
      " 22% 11/49 [00:26<01:45,  2.78s/it]\u001b[A\n",
      " 24% 12/49 [00:28<01:44,  2.81s/it]\u001b[A\n",
      " 27% 13/49 [00:31<01:41,  2.83s/it]\u001b[A\n",
      " 29% 14/49 [00:34<01:39,  2.85s/it]\u001b[A\n",
      " 31% 15/49 [00:37<01:37,  2.86s/it]\u001b[A\n",
      " 33% 16/49 [00:40<01:34,  2.86s/it]\u001b[A\n",
      " 35% 17/49 [00:43<01:31,  2.87s/it]\u001b[A\n",
      " 37% 18/49 [00:46<01:29,  2.87s/it]\u001b[A\n",
      " 39% 19/49 [00:49<01:26,  2.88s/it]\u001b[A\n",
      " 41% 20/49 [00:51<01:23,  2.88s/it]\u001b[A\n",
      " 43% 21/49 [00:54<01:20,  2.88s/it]\u001b[A\n",
      " 45% 22/49 [00:57<01:17,  2.88s/it]\u001b[A\n",
      " 47% 23/49 [01:00<01:14,  2.88s/it]\u001b[A\n",
      " 49% 24/49 [01:03<01:12,  2.88s/it]\u001b[A\n",
      " 51% 25/49 [01:06<01:09,  2.88s/it]\u001b[A\n",
      " 53% 26/49 [01:09<01:06,  2.88s/it]\u001b[A\n",
      " 55% 27/49 [01:12<01:03,  2.88s/it]\u001b[A\n",
      " 57% 28/49 [01:15<01:00,  2.88s/it]\u001b[A\n",
      " 59% 29/49 [01:17<00:57,  2.88s/it]\u001b[A\n",
      " 61% 30/49 [01:20<00:54,  2.88s/it]\u001b[A\n",
      " 63% 31/49 [01:23<00:51,  2.88s/it]\u001b[A\n",
      " 65% 32/49 [01:26<00:49,  2.88s/it]\u001b[A\n",
      " 67% 33/49 [01:29<00:46,  2.88s/it]\u001b[A\n",
      " 69% 34/49 [01:32<00:43,  2.88s/it]\u001b[A\n",
      " 71% 35/49 [01:35<00:40,  2.88s/it]\u001b[A\n",
      " 73% 36/49 [01:38<00:37,  2.88s/it]\u001b[A\n",
      " 76% 37/49 [01:40<00:34,  2.89s/it]\u001b[A\n",
      " 78% 38/49 [01:43<00:31,  2.88s/it]\u001b[A\n",
      " 80% 39/49 [01:46<00:28,  2.88s/it]\u001b[A\n",
      " 82% 40/49 [01:49<00:25,  2.89s/it]\u001b[A\n",
      " 84% 41/49 [01:52<00:23,  2.88s/it]\u001b[A\n",
      " 86% 42/49 [01:55<00:20,  2.89s/it]\u001b[A\n",
      " 88% 43/49 [01:58<00:17,  2.89s/it]\u001b[A\n",
      " 90% 44/49 [02:01<00:14,  2.89s/it]\u001b[A\n",
      " 92% 45/49 [02:04<00:11,  2.89s/it]\u001b[A\n",
      " 94% 46/49 [02:06<00:08,  2.88s/it]\u001b[A\n",
      " 96% 47/49 [02:09<00:05,  2.89s/it]\u001b[A\n",
      " 98% 48/49 [02:12<00:02,  2.88s/it]\u001b[A\n",
      "                                       \n",
      "\u001b[A{'eval_loss': 2.148526430130005, 'eval_runtime': 141.1657, 'eval_samples_per_second': 2.77, 'epoch': 1.0}\n",
      "100% 3527/3527 [42:04<00:00,  1.48it/s]\n",
      "100% 49/49 [02:18<00:00,  2.88s/it]\u001b[A\n",
      "                                   \u001b[A[INFO|trainer.py:1011] 2021-02-10 02:06:52,198 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "02/10/2021 02:06:52 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2524.9175, 'train_samples_per_second': 1.397, 'epoch': 1.0}\n",
      "100% 3527/3527 [42:04<00:00,  1.40it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-10 02:06:52,200 >> Saving model checkpoint to <Path to your output dir>\n",
      "02/10/2021 02:06:52 - INFO - transformers.trainer -   Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-10 02:06:52,203 >> Configuration saved in <Path to your output dir>/config.json\n",
      "02/10/2021 02:06:52 - INFO - transformers.configuration_utils -   Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-10 02:06:57,474 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/10/2021 02:06:57 - INFO - transformers.modeling_utils -   Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/10/2021 02:06:57 - INFO - __main__ -   ***** Train results *****\n",
      "02/10/2021 02:06:57 - INFO - __main__ -     epoch = 1.0\n",
      "02/10/2021 02:06:57 - INFO - __main__ -     train_runtime = 2524.9175\n",
      "02/10/2021 02:06:57 - INFO - __main__ -     train_samples_per_second = 1.397\n",
      "02/10/2021 02:06:57 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-10 02:06:57,551 >> ***** Running Evaluation *****\n",
      "02/10/2021 02:06:57 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 02:06:57,551 >>   Num examples = 391\n",
      "02/10/2021 02:06:57 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 02:06:57,551 >>   Batch size = 8\n",
      "02/10/2021 02:06:57 - INFO - transformers.trainer -     Batch size = 8\n",
      "100% 49/49 [02:20<00:00,  2.87s/it]\n",
      "02/10/2021 02:09:18 - INFO - __main__ -   ***** Eval results *****\n",
      "02/10/2021 02:09:18 - INFO - __main__ -     perplexity = 8.572217324544926\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 1 \\\n",
    "--fp16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc850DIAs3az"
   },
   "source": [
    "### Fine tuning with **3 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WAUB7Bsgs850",
    "outputId": "50c94e30-f998-448f-a1cb-e93357a0b26f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-10 02:10:09.675636: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/10/2021 02:10:11 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/10/2021 02:10:11 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb10_02-10-11_db3590ecbfe2, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 02:10:12,244 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 02:10:12 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 02:10:12,245 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 02:10:12 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 02:10:12,449 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 02:10:12 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 02:10:12,450 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 02:10:12 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 02:10:13,095 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 02:10:13 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 02:10:13,095 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 02:10:13 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 02:10:13,095 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 02:10:13 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 02:10:13,374 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 02:10:13 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 02:11:06,718 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "02/10/2021 02:11:06 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 02:11:06,718 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "02/10/2021 02:11:06 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-10 02:11:20,397 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3612570 > 1024). Running this sequence through the model will result in indexing errors\n",
      "02/10/2021 02:11:20 - WARNING - transformers.tokenization_utils_base -   Token indices sequence length is longer than the specified maximum sequence length for this model (3612570 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-baa35b79ab07d254.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-038d09c41662220c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-e9f9d50697755307.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-d3dc6fc336e64e87.arrow\n",
      "[INFO|trainer.py:432] 2021-02-10 02:11:39,834 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-10 02:11:39,834 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-10 02:11:39,835 >> Using amp fp16 backend\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -   Using amp fp16 backend\n",
      "[WARNING|training_args.py:514] 2021-02-10 02:11:39,835 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 02:11:39 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:514] 2021-02-10 02:11:39,841 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 02:11:39 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-10 02:11:39,841 >> ***** Running training *****\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -   ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 02:11:39,841 >>   Num examples = 3527\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -     Num examples = 3527\n",
      "[INFO|trainer.py:839] 2021-02-10 02:11:39,841 >>   Num Epochs = 3\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "[INFO|trainer.py:840] 2021-02-10 02:11:39,841 >>   Instantaneous batch size per device = 8\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-10 02:11:39,841 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-10 02:11:39,841 >>   Gradient Accumulation steps = 1\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-10 02:11:39,841 >>   Total optimization steps = 10581\n",
      "02/10/2021 02:11:39 - INFO - transformers.trainer -     Total optimization steps = 10581\n",
      "[WARNING|training_args.py:514] 2021-02-10 02:11:39,850 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 02:11:39 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/10581 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "{'loss': 2.4589, 'learning_rate': 5e-05, 'epoch': 0.14}\n",
      "{'loss': 2.3619, 'learning_rate': 5e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3225, 'learning_rate': 5e-05, 'epoch': 0.43}\n",
      "{'loss': 2.2954, 'learning_rate': 5e-05, 'epoch': 0.57}\n",
      "{'loss': 2.2796, 'learning_rate': 5e-05, 'epoch': 0.71}\n",
      "{'loss': 2.2542, 'learning_rate': 5e-05, 'epoch': 0.85}\n",
      "{'loss': 2.2348, 'learning_rate': 5e-05, 'epoch': 0.99}\n",
      " 33% 3527/10581 [39:47<1:19:52,  1.47it/s][INFO|trainer.py:1614] 2021-02-10 02:51:27,670 >> ***** Running Evaluation *****\n",
      "02/10/2021 02:51:27 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 02:51:27,670 >>   Num examples = 391\n",
      "02/10/2021 02:51:27 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 02:51:27,671 >>   Batch size = 8\n",
      "02/10/2021 02:51:27 - INFO - transformers.trainer -     Batch size = 8\n",
      "\n",
      "  0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 3/49 [00:02<00:44,  1.03it/s]\u001b[A\n",
      "  8% 4/49 [00:05<01:09,  1.54s/it]\u001b[A\n",
      " 10% 5/49 [00:08<01:25,  1.95s/it]\u001b[A\n",
      " 12% 6/49 [00:11<01:35,  2.23s/it]\u001b[A\n",
      " 14% 7/49 [00:14<01:41,  2.43s/it]\u001b[A\n",
      " 16% 8/49 [00:17<01:45,  2.57s/it]\u001b[A\n",
      " 18% 9/49 [00:20<01:46,  2.67s/it]\u001b[A\n",
      " 20% 10/49 [00:23<01:46,  2.73s/it]\u001b[A\n",
      " 22% 11/49 [00:26<01:45,  2.78s/it]\u001b[A\n",
      " 24% 12/49 [00:28<01:44,  2.82s/it]\u001b[A\n",
      " 27% 13/49 [00:31<01:42,  2.84s/it]\u001b[A\n",
      " 29% 14/49 [00:34<01:39,  2.86s/it]\u001b[A\n",
      " 31% 15/49 [00:37<01:37,  2.87s/it]\u001b[A\n",
      " 33% 16/49 [00:40<01:34,  2.88s/it]\u001b[A\n",
      " 35% 17/49 [00:43<01:32,  2.88s/it]\u001b[A\n",
      " 37% 18/49 [00:46<01:29,  2.88s/it]\u001b[A\n",
      " 39% 19/49 [00:49<01:26,  2.89s/it]\u001b[A\n",
      " 41% 20/49 [00:52<01:23,  2.88s/it]\u001b[A\n",
      " 43% 21/49 [00:54<01:20,  2.89s/it]\u001b[A\n",
      " 45% 22/49 [00:57<01:17,  2.88s/it]\u001b[A\n",
      " 47% 23/49 [01:00<01:14,  2.88s/it]\u001b[A\n",
      " 49% 24/49 [01:03<01:12,  2.89s/it]\u001b[A\n",
      " 51% 25/49 [01:06<01:09,  2.88s/it]\u001b[A\n",
      " 53% 26/49 [01:09<01:06,  2.88s/it]\u001b[A\n",
      " 55% 27/49 [01:12<01:03,  2.88s/it]\u001b[A\n",
      " 57% 28/49 [01:15<01:00,  2.88s/it]\u001b[A\n",
      " 59% 29/49 [01:18<00:57,  2.88s/it]\u001b[A\n",
      " 61% 30/49 [01:20<00:54,  2.88s/it]\u001b[A\n",
      " 63% 31/49 [01:23<00:51,  2.88s/it]\u001b[A\n",
      " 65% 32/49 [01:26<00:48,  2.88s/it]\u001b[A\n",
      " 67% 33/49 [01:29<00:46,  2.88s/it]\u001b[A\n",
      " 69% 34/49 [01:32<00:43,  2.88s/it]\u001b[A\n",
      " 71% 35/49 [01:35<00:40,  2.88s/it]\u001b[A\n",
      " 73% 36/49 [01:38<00:37,  2.88s/it]\u001b[A\n",
      " 76% 37/49 [01:41<00:34,  2.88s/it]\u001b[A\n",
      " 78% 38/49 [01:43<00:31,  2.88s/it]\u001b[A\n",
      " 80% 39/49 [01:46<00:28,  2.88s/it]\u001b[A\n",
      " 82% 40/49 [01:49<00:25,  2.88s/it]\u001b[A\n",
      " 84% 41/49 [01:52<00:22,  2.87s/it]\u001b[A\n",
      " 86% 42/49 [01:55<00:20,  2.88s/it]\u001b[A\n",
      " 88% 43/49 [01:58<00:17,  2.88s/it]\u001b[A\n",
      " 90% 44/49 [02:01<00:14,  2.88s/it]\u001b[A\n",
      " 92% 45/49 [02:04<00:11,  2.87s/it]\u001b[A\n",
      " 94% 46/49 [02:06<00:08,  2.87s/it]\u001b[A\n",
      " 96% 47/49 [02:09<00:05,  2.87s/it]\u001b[A\n",
      " 98% 48/49 [02:12<00:02,  2.87s/it]\u001b[A\n",
      "                                          \n",
      "\u001b[A{'eval_loss': 2.148526430130005, 'eval_runtime': 141.1024, 'eval_samples_per_second': 2.771, 'epoch': 1.0}\n",
      " 33% 3527/10581 [42:08<1:19:52,  1.47it/s]\n",
      "100% 49/49 [02:18<00:00,  2.87s/it]\u001b[A\n",
      "{'loss': 2.0227, 'learning_rate': 5e-05, 'epoch': 1.13}\n",
      "{'loss': 2.0273, 'learning_rate': 5e-05, 'epoch': 1.28}\n",
      "{'loss': 2.0265, 'learning_rate': 5e-05, 'epoch': 1.42}\n",
      "{'loss': 2.0103, 'learning_rate': 5e-05, 'epoch': 1.56}\n",
      "{'loss': 2.0175, 'learning_rate': 5e-05, 'epoch': 1.7}\n",
      "{'loss': 2.0127, 'learning_rate': 5e-05, 'epoch': 1.84}\n",
      "{'loss': 1.9982, 'learning_rate': 5e-05, 'epoch': 1.98}\n",
      " 67% 7054/10581 [1:22:01<39:43,  1.48it/s][INFO|trainer.py:1614] 2021-02-10 03:33:41,186 >> ***** Running Evaluation *****\n",
      "02/10/2021 03:33:41 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 03:33:41,186 >>   Num examples = 391\n",
      "02/10/2021 03:33:41 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 03:33:41,186 >>   Batch size = 8\n",
      "02/10/2021 03:33:41 - INFO - transformers.trainer -     Batch size = 8\n",
      "\n",
      "  0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 2/49 [00:02<01:07,  1.45s/it]\u001b[A\n",
      "  6% 3/49 [00:05<01:26,  1.87s/it]\u001b[A\n",
      "  8% 4/49 [00:08<01:37,  2.17s/it]\u001b[A\n",
      " 10% 5/49 [00:11<01:44,  2.38s/it]\u001b[A\n",
      " 12% 6/49 [00:14<01:48,  2.52s/it]\u001b[A\n",
      " 14% 7/49 [00:17<01:50,  2.62s/it]\u001b[A\n",
      " 16% 8/49 [00:20<01:50,  2.69s/it]\u001b[A\n",
      " 18% 9/49 [00:22<01:49,  2.75s/it]\u001b[A\n",
      " 20% 10/49 [00:25<01:48,  2.78s/it]\u001b[A\n",
      " 22% 11/49 [00:28<01:46,  2.80s/it]\u001b[A\n",
      " 24% 12/49 [00:31<01:44,  2.81s/it]\u001b[A\n",
      " 27% 13/49 [00:34<01:41,  2.82s/it]\u001b[A\n",
      " 29% 14/49 [00:37<01:38,  2.83s/it]\u001b[A\n",
      " 31% 15/49 [00:40<01:36,  2.83s/it]\u001b[A\n",
      " 33% 16/49 [00:42<01:33,  2.84s/it]\u001b[A\n",
      " 35% 17/49 [00:45<01:31,  2.85s/it]\u001b[A\n",
      " 37% 18/49 [00:48<01:28,  2.85s/it]\u001b[A\n",
      " 39% 19/49 [00:51<01:25,  2.86s/it]\u001b[A\n",
      " 41% 20/49 [00:54<01:22,  2.86s/it]\u001b[A\n",
      " 43% 21/49 [00:57<01:20,  2.86s/it]\u001b[A\n",
      " 45% 22/49 [01:00<01:17,  2.87s/it]\u001b[A\n",
      " 47% 23/49 [01:02<01:14,  2.87s/it]\u001b[A\n",
      " 49% 24/49 [01:05<01:11,  2.87s/it]\u001b[A\n",
      " 51% 25/49 [01:08<01:08,  2.87s/it]\u001b[A\n",
      " 53% 26/49 [01:11<01:06,  2.88s/it]\u001b[A\n",
      " 55% 27/49 [01:14<01:03,  2.88s/it]\u001b[A\n",
      " 57% 28/49 [01:17<01:00,  2.88s/it]\u001b[A\n",
      " 59% 29/49 [01:20<00:57,  2.88s/it]\u001b[A\n",
      " 61% 30/49 [01:23<00:54,  2.88s/it]\u001b[A\n",
      " 63% 31/49 [01:26<00:51,  2.88s/it]\u001b[A\n",
      " 65% 32/49 [01:28<00:49,  2.88s/it]\u001b[A\n",
      " 67% 33/49 [01:31<00:46,  2.88s/it]\u001b[A\n",
      " 69% 34/49 [01:34<00:43,  2.88s/it]\u001b[A\n",
      " 71% 35/49 [01:37<00:40,  2.88s/it]\u001b[A\n",
      " 73% 36/49 [01:40<00:37,  2.88s/it]\u001b[A\n",
      " 76% 37/49 [01:43<00:34,  2.88s/it]\u001b[A\n",
      " 78% 38/49 [01:46<00:31,  2.87s/it]\u001b[A\n",
      " 80% 39/49 [01:49<00:28,  2.87s/it]\u001b[A\n",
      " 82% 40/49 [01:51<00:25,  2.87s/it]\u001b[A\n",
      " 84% 41/49 [01:54<00:22,  2.86s/it]\u001b[A\n",
      " 86% 42/49 [01:57<00:20,  2.86s/it]\u001b[A\n",
      " 88% 43/49 [02:00<00:17,  2.86s/it]\u001b[A\n",
      " 90% 44/49 [02:03<00:14,  2.85s/it]\u001b[A\n",
      " 92% 45/49 [02:06<00:11,  2.85s/it]\u001b[A\n",
      " 94% 46/49 [02:08<00:08,  2.85s/it]\u001b[A\n",
      " 96% 47/49 [02:11<00:05,  2.85s/it]\u001b[A\n",
      " 98% 48/49 [02:14<00:02,  2.86s/it]\u001b[A\n",
      "                                          \n",
      "\u001b[A{'eval_loss': 2.0794761180877686, 'eval_runtime': 140.0787, 'eval_samples_per_second': 2.791, 'epoch': 2.0}\n",
      " 67% 7054/10581 [1:24:21<39:43,  1.48it/s]\n",
      "100% 49/49 [02:20<00:00,  2.86s/it]\u001b[A\n",
      "{'loss': 1.8226, 'learning_rate': 5e-05, 'epoch': 2.13}\n",
      "{'loss': 1.8039, 'learning_rate': 5e-05, 'epoch': 2.27}\n",
      "{'loss': 1.8087, 'learning_rate': 5e-05, 'epoch': 2.41}\n",
      "{'loss': 1.8035, 'learning_rate': 5e-05, 'epoch': 2.55}\n",
      "{'loss': 1.8155, 'learning_rate': 5e-05, 'epoch': 2.69}\n",
      "{'loss': 1.8027, 'learning_rate': 5e-05, 'epoch': 2.84}\n",
      "{'loss': 1.8022, 'learning_rate': 5e-05, 'epoch': 2.98}\n",
      "100% 10581/10581 [2:04:15<00:00,  1.48it/s][INFO|trainer.py:1614] 2021-02-10 04:15:55,354 >> ***** Running Evaluation *****\n",
      "02/10/2021 04:15:55 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 04:15:55,354 >>   Num examples = 391\n",
      "02/10/2021 04:15:55 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 04:15:55,354 >>   Batch size = 8\n",
      "02/10/2021 04:15:55 - INFO - transformers.trainer -     Batch size = 8\n",
      "\n",
      "  0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 2/49 [00:02<01:07,  1.44s/it]\u001b[A\n",
      "  6% 3/49 [00:05<01:25,  1.86s/it]\u001b[A\n",
      "  8% 4/49 [00:08<01:37,  2.16s/it]\u001b[A\n",
      " 10% 5/49 [00:11<01:44,  2.37s/it]\u001b[A\n",
      " 12% 6/49 [00:14<01:48,  2.52s/it]\u001b[A\n",
      " 14% 7/49 [00:17<01:50,  2.62s/it]\u001b[A\n",
      " 16% 8/49 [00:20<01:50,  2.69s/it]\u001b[A\n",
      " 18% 9/49 [00:22<01:49,  2.74s/it]\u001b[A\n",
      " 20% 10/49 [00:25<01:48,  2.78s/it]\u001b[A\n",
      " 22% 11/49 [00:28<01:46,  2.80s/it]\u001b[A\n",
      " 24% 12/49 [00:31<01:44,  2.82s/it]\u001b[A\n",
      " 27% 13/49 [00:34<01:42,  2.84s/it]\u001b[A\n",
      " 29% 14/49 [00:37<01:39,  2.84s/it]\u001b[A\n",
      " 31% 15/49 [00:40<01:36,  2.85s/it]\u001b[A\n",
      " 33% 16/49 [00:42<01:34,  2.85s/it]\u001b[A\n",
      " 35% 17/49 [00:45<01:31,  2.86s/it]\u001b[A\n",
      " 37% 18/49 [00:48<01:28,  2.86s/it]\u001b[A\n",
      " 39% 19/49 [00:51<01:25,  2.86s/it]\u001b[A\n",
      " 41% 20/49 [00:54<01:22,  2.86s/it]\u001b[A\n",
      " 43% 21/49 [00:57<01:20,  2.86s/it]\u001b[A\n",
      " 45% 22/49 [01:00<01:17,  2.86s/it]\u001b[A\n",
      " 47% 23/49 [01:02<01:14,  2.86s/it]\u001b[A\n",
      " 49% 24/49 [01:05<01:11,  2.86s/it]\u001b[A\n",
      " 51% 25/49 [01:08<01:08,  2.87s/it]\u001b[A\n",
      " 53% 26/49 [01:11<01:05,  2.87s/it]\u001b[A\n",
      " 55% 27/49 [01:14<01:03,  2.86s/it]\u001b[A\n",
      " 57% 28/49 [01:17<01:00,  2.86s/it]\u001b[A\n",
      " 59% 29/49 [01:20<00:57,  2.87s/it]\u001b[A\n",
      " 61% 30/49 [01:23<00:54,  2.87s/it]\u001b[A\n",
      " 63% 31/49 [01:25<00:51,  2.87s/it]\u001b[A\n",
      " 65% 32/49 [01:28<00:48,  2.87s/it]\u001b[A\n",
      " 67% 33/49 [01:31<00:45,  2.87s/it]\u001b[A\n",
      " 69% 34/49 [01:34<00:43,  2.87s/it]\u001b[A\n",
      " 71% 35/49 [01:37<00:40,  2.87s/it]\u001b[A\n",
      " 73% 36/49 [01:40<00:37,  2.87s/it]\u001b[A\n",
      " 76% 37/49 [01:43<00:34,  2.87s/it]\u001b[A\n",
      " 78% 38/49 [01:46<00:31,  2.87s/it]\u001b[A\n",
      " 80% 39/49 [01:48<00:28,  2.87s/it]\u001b[A\n",
      " 82% 40/49 [01:51<00:25,  2.87s/it]\u001b[A\n",
      " 84% 41/49 [01:54<00:22,  2.87s/it]\u001b[A\n",
      " 86% 42/49 [01:57<00:20,  2.87s/it]\u001b[A\n",
      " 88% 43/49 [02:00<00:17,  2.87s/it]\u001b[A\n",
      " 90% 44/49 [02:03<00:14,  2.87s/it]\u001b[A\n",
      " 92% 45/49 [02:06<00:11,  2.87s/it]\u001b[A\n",
      " 94% 46/49 [02:08<00:08,  2.87s/it]\u001b[A\n",
      " 96% 47/49 [02:11<00:05,  2.87s/it]\u001b[A\n",
      " 98% 48/49 [02:14<00:02,  2.87s/it]\u001b[A\n",
      "                                           \n",
      "\u001b[A{'eval_loss': 2.0590474605560303, 'eval_runtime': 140.0773, 'eval_samples_per_second': 2.791, 'epoch': 3.0}\n",
      "100% 10581/10581 [2:06:35<00:00,  1.48it/s]\n",
      "100% 49/49 [02:20<00:00,  2.87s/it]\u001b[A\n",
      "                                   \u001b[A[INFO|trainer.py:1011] 2021-02-10 04:18:15,433 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "02/10/2021 04:18:15 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 7595.5917, 'train_samples_per_second': 1.393, 'epoch': 3.0}\n",
      "100% 10581/10581 [2:06:35<00:00,  1.39it/s]\n",
      "[INFO|trainer.py:1412] 2021-02-10 04:18:15,434 >> Saving model checkpoint to <Path to your output dir>\n",
      "02/10/2021 04:18:15 - INFO - transformers.trainer -   Saving model checkpoint to <Path to your output dir>\n",
      "[INFO|configuration_utils.py:304] 2021-02-10 04:18:15,437 >> Configuration saved in <Path to your output dir>/config.json\n",
      "02/10/2021 04:18:15 - INFO - transformers.configuration_utils -   Configuration saved in <Path to your output dir>/config.json\n",
      "[INFO|modeling_utils.py:817] 2021-02-10 04:18:20,983 >> Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/10/2021 04:18:20 - INFO - transformers.modeling_utils -   Model weights saved in <Path to your output dir>/pytorch_model.bin\n",
      "02/10/2021 04:18:25 - INFO - __main__ -   ***** Train results *****\n",
      "02/10/2021 04:18:25 - INFO - __main__ -     epoch = 3.0\n",
      "02/10/2021 04:18:25 - INFO - __main__ -     train_runtime = 7595.5917\n",
      "02/10/2021 04:18:25 - INFO - __main__ -     train_samples_per_second = 1.393\n",
      "02/10/2021 04:18:25 - INFO - __main__ -   *** Evaluate ***\n",
      "[INFO|trainer.py:1614] 2021-02-10 04:18:25,286 >> ***** Running Evaluation *****\n",
      "02/10/2021 04:18:25 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 04:18:25,286 >>   Num examples = 391\n",
      "02/10/2021 04:18:25 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 04:18:25,286 >>   Batch size = 8\n",
      "02/10/2021 04:18:25 - INFO - transformers.trainer -     Batch size = 8\n",
      "100% 49/49 [02:20<00:00,  2.87s/it]\n",
      "02/10/2021 04:20:46 - INFO - __main__ -   ***** Eval results *****\n",
      "02/10/2021 04:20:46 - INFO - __main__ -     perplexity = 7.8384997729153785\n"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 3 \\\n",
    "--fp16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-aNrkxFLEmF"
   },
   "source": [
    "### Fine tuning with **5 epochs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PlVMTR4yLBUO",
    "outputId": "79983f0b-dabf-4755-82dd-ddac1d694ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-10 04:21:35.962423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "02/10/2021 04:21:37 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "02/10/2021 04:21:37 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=<Path to your output dir>, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, lr_scheduler_type=SchedulerType.CONSTANT, warmup_steps=0, logging_dir=runs/Feb10_04-21-37_db3590ecbfe2, logging_first_step=False, logging_steps=500, save_steps=-1, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=<Path to your output dir>, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)\n",
      "Using custom data configuration default\n",
      "Reusing dataset text (/root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab)\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 04:21:38,247 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 04:21:38 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 04:21:38,248 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 04:21:38 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:449] 2021-02-10 04:21:38,453 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "02/10/2021 04:21:38 - INFO - transformers.configuration_utils -   loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
      "[INFO|configuration_utils.py:485] 2021-02-10 04:21:38,454 >> Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "02/10/2021 04:21:38 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 1024,\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 1024,\n",
      "  \"n_special\": 0,\n",
      "  \"predict_special_tokens\": true,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 04:21:39,230 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "02/10/2021 04:21:39 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 04:21:39,230 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "02/10/2021 04:21:39 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1786] 2021-02-10 04:21:39,230 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "02/10/2021 04:21:39 - INFO - transformers.tokenization_utils_base -   loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "[INFO|modeling_utils.py:1027] 2021-02-10 04:21:39,507 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "02/10/2021 04:21:39 - INFO - transformers.modeling_utils -   loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
      "[INFO|modeling_utils.py:1143] 2021-02-10 04:22:28,156 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "02/10/2021 04:22:28 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "[INFO|modeling_utils.py:1152] 2021-02-10 04:22:28,156 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "02/10/2021 04:22:28 - INFO - transformers.modeling_utils -   All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "[WARNING|tokenization_utils_base.py:3213] 2021-02-10 04:22:41,622 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3612570 > 1024). Running this sequence through the model will result in indexing errors\n",
      "02/10/2021 04:22:41 - WARNING - transformers.tokenization_utils_base -   Token indices sequence length is longer than the specified maximum sequence length for this model (3612570 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-baa35b79ab07d254.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-038d09c41662220c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-e9f9d50697755307.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/text/default-3fae79341c5451f5/0.0.0/daf90a707a433ac193b369c8cc1772139bb6cca21a9c7fe83bdd16aad9b9b6ab/cache-d3dc6fc336e64e87.arrow\n",
      "[INFO|trainer.py:432] 2021-02-10 04:23:01,247 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -   The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:432] 2021-02-10 04:23:01,247 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -   The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
      "[INFO|trainer.py:348] 2021-02-10 04:23:01,248 >> Using amp fp16 backend\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -   Using amp fp16 backend\n",
      "[WARNING|training_args.py:514] 2021-02-10 04:23:01,248 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 04:23:01 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:514] 2021-02-10 04:23:01,253 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 04:23:01 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:837] 2021-02-10 04:23:01,254 >> ***** Running training *****\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -   ***** Running training *****\n",
      "[INFO|trainer.py:838] 2021-02-10 04:23:01,254 >>   Num examples = 3527\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -     Num examples = 3527\n",
      "[INFO|trainer.py:839] 2021-02-10 04:23:01,254 >>   Num Epochs = 5\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -     Num Epochs = 5\n",
      "[INFO|trainer.py:840] 2021-02-10 04:23:01,254 >>   Instantaneous batch size per device = 8\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:841] 2021-02-10 04:23:01,254 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "[INFO|trainer.py:842] 2021-02-10 04:23:01,254 >>   Gradient Accumulation steps = 1\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:843] 2021-02-10 04:23:01,254 >>   Total optimization steps = 17635\n",
      "02/10/2021 04:23:01 - INFO - transformers.trainer -     Total optimization steps = 17635\n",
      "[WARNING|training_args.py:514] 2021-02-10 04:23:01,261 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "02/10/2021 04:23:01 - WARNING - transformers.training_args -   Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "  0% 0/17635 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "{'loss': 2.4589, 'learning_rate': 5e-05, 'epoch': 0.14}\n",
      "{'loss': 2.3619, 'learning_rate': 5e-05, 'epoch': 0.28}\n",
      "{'loss': 2.3225, 'learning_rate': 5e-05, 'epoch': 0.43}\n",
      "{'loss': 2.2954, 'learning_rate': 5e-05, 'epoch': 0.57}\n",
      "{'loss': 2.2796, 'learning_rate': 5e-05, 'epoch': 0.71}\n",
      "{'loss': 2.2542, 'learning_rate': 5e-05, 'epoch': 0.85}\n",
      "{'loss': 2.2348, 'learning_rate': 5e-05, 'epoch': 0.99}\n",
      " 20% 3527/17635 [39:53<2:39:28,  1.47it/s][INFO|trainer.py:1614] 2021-02-10 05:02:54,513 >> ***** Running Evaluation *****\n",
      "02/10/2021 05:02:54 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 05:02:54,513 >>   Num examples = 391\n",
      "02/10/2021 05:02:54 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 05:02:54,514 >>   Batch size = 8\n",
      "02/10/2021 05:02:54 - INFO - transformers.trainer -     Batch size = 8\n",
      "\n",
      "  0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  6% 3/49 [00:02<00:44,  1.04it/s]\u001b[A\n",
      "  8% 4/49 [00:05<01:09,  1.54s/it]\u001b[A\n",
      " 10% 5/49 [00:08<01:25,  1.94s/it]\u001b[A\n",
      " 12% 6/49 [00:11<01:35,  2.22s/it]\u001b[A\n",
      " 14% 7/49 [00:14<01:41,  2.41s/it]\u001b[A\n",
      " 16% 8/49 [00:17<01:44,  2.55s/it]\u001b[A\n",
      " 18% 9/49 [00:20<01:45,  2.64s/it]\u001b[A\n",
      " 20% 10/49 [00:22<01:45,  2.71s/it]\u001b[A\n",
      " 22% 11/49 [00:25<01:44,  2.76s/it]\u001b[A\n",
      " 24% 12/49 [00:28<01:43,  2.79s/it]\u001b[A\n",
      " 27% 13/49 [00:31<01:41,  2.81s/it]\u001b[A\n",
      " 29% 14/49 [00:34<01:39,  2.83s/it]\u001b[A\n",
      " 31% 15/49 [00:37<01:36,  2.84s/it]\u001b[A\n",
      " 33% 16/49 [00:40<01:33,  2.85s/it]\u001b[A\n",
      " 35% 17/49 [00:43<01:31,  2.85s/it]\u001b[A\n",
      " 37% 18/49 [00:45<01:28,  2.86s/it]\u001b[A\n",
      " 39% 19/49 [00:48<01:25,  2.86s/it]\u001b[A\n",
      " 41% 20/49 [00:51<01:22,  2.86s/it]\u001b[A\n",
      " 43% 21/49 [00:54<01:20,  2.86s/it]\u001b[A\n",
      " 45% 22/49 [00:57<01:17,  2.86s/it]\u001b[A\n",
      " 47% 23/49 [01:00<01:14,  2.86s/it]\u001b[A\n",
      " 49% 24/49 [01:03<01:11,  2.86s/it]\u001b[A\n",
      " 51% 25/49 [01:05<01:08,  2.86s/it]\u001b[A\n",
      " 53% 26/49 [01:08<01:05,  2.86s/it]\u001b[A\n",
      " 55% 27/49 [01:11<01:02,  2.86s/it]\u001b[A\n",
      " 57% 28/49 [01:14<01:00,  2.86s/it]\u001b[A\n",
      " 59% 29/49 [01:17<00:57,  2.86s/it]\u001b[A\n",
      " 61% 30/49 [01:20<00:54,  2.86s/it]\u001b[A\n",
      " 63% 31/49 [01:23<00:51,  2.86s/it]\u001b[A\n",
      " 65% 32/49 [01:25<00:48,  2.86s/it]\u001b[A\n",
      " 67% 33/49 [01:28<00:45,  2.86s/it]\u001b[A\n",
      " 69% 34/49 [01:31<00:42,  2.86s/it]\u001b[A\n",
      " 71% 35/49 [01:34<00:40,  2.86s/it]\u001b[A\n",
      " 73% 36/49 [01:37<00:37,  2.86s/it]\u001b[A\n",
      " 76% 37/49 [01:40<00:34,  2.86s/it]\u001b[A\n",
      " 78% 38/49 [01:43<00:31,  2.86s/it]\u001b[A\n",
      " 80% 39/49 [01:45<00:28,  2.86s/it]\u001b[A\n",
      " 82% 40/49 [01:48<00:25,  2.86s/it]\u001b[A\n",
      " 84% 41/49 [01:51<00:22,  2.86s/it]\u001b[A\n",
      " 86% 42/49 [01:54<00:20,  2.86s/it]\u001b[A\n",
      " 88% 43/49 [01:57<00:17,  2.86s/it]\u001b[A\n",
      " 90% 44/49 [02:00<00:14,  2.86s/it]\u001b[A\n",
      " 92% 45/49 [02:03<00:11,  2.86s/it]\u001b[A\n",
      " 94% 46/49 [02:05<00:08,  2.86s/it]\u001b[A\n",
      " 96% 47/49 [02:08<00:05,  2.86s/it]\u001b[A\n",
      " 98% 48/49 [02:11<00:02,  2.86s/it]\u001b[A\n",
      "                                          \n",
      "\u001b[A{'eval_loss': 2.148526430130005, 'eval_runtime': 140.064, 'eval_samples_per_second': 2.792, 'epoch': 1.0}\n",
      " 20% 3527/17635 [42:13<2:39:28,  1.47it/s]\n",
      "100% 49/49 [02:17<00:00,  2.86s/it]\u001b[A\n",
      "{'loss': 2.0227, 'learning_rate': 5e-05, 'epoch': 1.13}\n",
      "{'loss': 2.0273, 'learning_rate': 5e-05, 'epoch': 1.28}\n",
      "{'loss': 2.0265, 'learning_rate': 5e-05, 'epoch': 1.42}\n",
      "{'loss': 2.0103, 'learning_rate': 5e-05, 'epoch': 1.56}\n",
      "{'loss': 2.0175, 'learning_rate': 5e-05, 'epoch': 1.7}\n",
      "{'loss': 2.0127, 'learning_rate': 5e-05, 'epoch': 1.84}\n",
      "{'loss': 1.9982, 'learning_rate': 5e-05, 'epoch': 1.98}\n",
      " 40% 7054/17635 [1:22:09<2:00:14,  1.47it/s][INFO|trainer.py:1614] 2021-02-10 05:45:10,406 >> ***** Running Evaluation *****\n",
      "02/10/2021 05:45:10 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 05:45:10,406 >>   Num examples = 391\n",
      "02/10/2021 05:45:10 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 05:45:10,406 >>   Batch size = 8\n",
      "02/10/2021 05:45:10 - INFO - transformers.trainer -     Batch size = 8\n",
      "\n",
      "  0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 2/49 [00:02<01:07,  1.44s/it]\u001b[A\n",
      "  6% 3/49 [00:05<01:26,  1.88s/it]\u001b[A\n",
      "  8% 4/49 [00:08<01:38,  2.18s/it]\u001b[A\n",
      " 10% 5/49 [00:11<01:45,  2.39s/it]\u001b[A\n",
      " 12% 6/49 [00:14<01:49,  2.54s/it]\u001b[A\n",
      " 14% 7/49 [00:17<01:51,  2.64s/it]\u001b[A\n",
      " 16% 8/49 [00:20<01:51,  2.72s/it]\u001b[A\n",
      " 18% 9/49 [00:23<01:50,  2.77s/it]\u001b[A\n",
      " 20% 10/49 [00:25<01:49,  2.80s/it]\u001b[A\n",
      " 22% 11/49 [00:28<01:47,  2.83s/it]\u001b[A\n",
      " 24% 12/49 [00:31<01:45,  2.84s/it]\u001b[A\n",
      " 27% 13/49 [00:34<01:42,  2.85s/it]\u001b[A\n",
      " 29% 14/49 [00:37<01:40,  2.86s/it]\u001b[A\n",
      " 31% 15/49 [00:40<01:37,  2.86s/it]\u001b[A\n",
      " 33% 16/49 [00:43<01:34,  2.86s/it]\u001b[A\n",
      " 35% 17/49 [00:46<01:31,  2.86s/it]\u001b[A\n",
      " 37% 18/49 [00:48<01:28,  2.86s/it]\u001b[A\n",
      " 39% 19/49 [00:51<01:25,  2.85s/it]\u001b[A\n",
      " 41% 20/49 [00:54<01:22,  2.86s/it]\u001b[A\n",
      " 43% 21/49 [00:57<01:19,  2.85s/it]\u001b[A\n",
      " 45% 22/49 [01:00<01:17,  2.86s/it]\u001b[A\n",
      " 47% 23/49 [01:03<01:14,  2.86s/it]\u001b[A\n",
      " 49% 24/49 [01:06<01:11,  2.86s/it]\u001b[A\n",
      " 51% 25/49 [01:08<01:08,  2.86s/it]\u001b[A\n",
      " 53% 26/49 [01:11<01:05,  2.86s/it]\u001b[A\n",
      " 55% 27/49 [01:14<01:03,  2.87s/it]\u001b[A\n",
      " 57% 28/49 [01:17<01:00,  2.87s/it]\u001b[A\n",
      " 59% 29/49 [01:20<00:57,  2.87s/it]\u001b[A\n",
      " 61% 30/49 [01:23<00:54,  2.87s/it]\u001b[A\n",
      " 63% 31/49 [01:26<00:51,  2.87s/it]\u001b[A\n",
      " 65% 32/49 [01:29<00:48,  2.87s/it]\u001b[A\n",
      " 67% 33/49 [01:31<00:46,  2.88s/it]\u001b[A\n",
      " 69% 34/49 [01:34<00:43,  2.88s/it]\u001b[A\n",
      " 71% 35/49 [01:37<00:40,  2.88s/it]\u001b[A\n",
      " 73% 36/49 [01:40<00:37,  2.88s/it]\u001b[A\n",
      " 76% 37/49 [01:43<00:34,  2.89s/it]\u001b[A\n",
      " 78% 38/49 [01:46<00:31,  2.89s/it]\u001b[A\n",
      " 80% 39/49 [01:49<00:28,  2.89s/it]\u001b[A\n",
      " 82% 40/49 [01:52<00:25,  2.88s/it]\u001b[A\n",
      " 84% 41/49 [01:55<00:23,  2.88s/it]\u001b[A\n",
      " 86% 42/49 [01:57<00:20,  2.88s/it]\u001b[A\n",
      " 88% 43/49 [02:00<00:17,  2.88s/it]\u001b[A\n",
      " 90% 44/49 [02:03<00:14,  2.87s/it]\u001b[A\n",
      " 92% 45/49 [02:06<00:11,  2.87s/it]\u001b[A\n",
      " 94% 46/49 [02:09<00:08,  2.87s/it]\u001b[A\n",
      " 96% 47/49 [02:12<00:05,  2.87s/it]\u001b[A\n",
      " 98% 48/49 [02:15<00:02,  2.86s/it]\u001b[A\n",
      "                                            \n",
      "\u001b[A{'eval_loss': 2.0794761180877686, 'eval_runtime': 140.4493, 'eval_samples_per_second': 2.784, 'epoch': 2.0}\n",
      " 40% 7054/17635 [1:24:29<2:00:14,  1.47it/s]\n",
      "100% 49/49 [02:20<00:00,  2.86s/it]\u001b[A\n",
      "{'loss': 1.8226, 'learning_rate': 5e-05, 'epoch': 2.13}\n",
      "{'loss': 1.8039, 'learning_rate': 5e-05, 'epoch': 2.27}\n",
      "{'loss': 1.8087, 'learning_rate': 5e-05, 'epoch': 2.41}\n",
      "{'loss': 1.8035, 'learning_rate': 5e-05, 'epoch': 2.55}\n",
      "{'loss': 1.8155, 'learning_rate': 5e-05, 'epoch': 2.69}\n",
      "{'loss': 1.8027, 'learning_rate': 5e-05, 'epoch': 2.84}\n",
      "{'loss': 1.8022, 'learning_rate': 5e-05, 'epoch': 2.98}\n",
      " 60% 10581/17635 [2:04:24<1:19:30,  1.48it/s][INFO|trainer.py:1614] 2021-02-10 06:27:25,722 >> ***** Running Evaluation *****\n",
      "02/10/2021 06:27:25 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1615] 2021-02-10 06:27:25,722 >>   Num examples = 391\n",
      "02/10/2021 06:27:25 - INFO - transformers.trainer -     Num examples = 391\n",
      "[INFO|trainer.py:1616] 2021-02-10 06:27:25,722 >>   Batch size = 8\n",
      "02/10/2021 06:27:25 - INFO - transformers.trainer -     Batch size = 8\n",
      "\n",
      "  0% 0/49 [00:00<?, ?it/s]\u001b[A\n",
      "  4% 2/49 [00:02<01:07,  1.43s/it]\u001b[A\n",
      "  6% 3/49 [00:05<01:25,  1.86s/it]\u001b[A\n",
      "  8% 4/49 [00:08<01:37,  2.16s/it]\u001b[A\n",
      " 10% 5/49 [00:11<01:44,  2.38s/it]\u001b[A\n",
      " 12% 6/49 [00:14<01:48,  2.53s/it]\u001b[A\n",
      " 14% 7/49 [00:17<01:50,  2.63s/it]\u001b[A\n",
      " 16% 8/49 [00:20<01:50,  2.70s/it]\u001b[A\n",
      " 18% 9/49 [00:22<01:50,  2.75s/it]\u001b[A\n",
      " 20% 10/49 [00:25<01:48,  2.79s/it]\u001b[A\n",
      " 22% 11/49 [00:28<01:46,  2.81s/it]\u001b[A\n",
      " 24% 12/49 [00:31<01:44,  2.83s/it]\u001b[A\n",
      " 27% 13/49 [00:34<01:42,  2.84s/it]\u001b[A\n",
      " 29% 14/49 [00:37<01:39,  2.85s/it]\u001b[A\n",
      " 31% 15/49 [00:40<01:37,  2.86s/it]\u001b[A\n",
      " 33% 16/49 [00:43<01:34,  2.86s/it]\u001b[A\n",
      " 35% 17/49 [00:45<01:31,  2.86s/it]\u001b[A\n",
      " 37% 18/49 [00:48<01:28,  2.87s/it]\u001b[A\n",
      " 39% 19/49 [00:51<01:26,  2.87s/it]\u001b[A\n",
      " 41% 20/49 [00:54<01:23,  2.87s/it]\u001b[A\n",
      " 43% 21/49 [00:57<01:20,  2.87s/it]\u001b[A\n",
      " 45% 22/49 [01:00<01:17,  2.87s/it]\u001b[A\n",
      " 47% 23/49 [01:03<01:14,  2.87s/it]\u001b[A\n",
      " 49% 24/49 [01:06<01:11,  2.87s/it]\u001b[A\n",
      " 51% 25/49 [01:08<01:08,  2.87s/it]\u001b[A\n",
      " 53% 26/49 [01:11<01:06,  2.87s/it]\u001b[A\n",
      " 55% 27/49 [01:14<01:03,  2.87s/it]\u001b[A\n",
      " 57% 28/49 [01:17<01:00,  2.87s/it]\u001b[A\n",
      " 59% 29/49 [01:20<00:57,  2.87s/it]\u001b[A\n",
      " 61% 30/49 [01:23<00:54,  2.87s/it]\u001b[A\n",
      " 63% 31/49 [01:26<00:51,  2.87s/it]\u001b[A\n",
      " 65% 32/49 [01:29<00:48,  2.87s/it]\u001b[A\n",
      " 67% 33/49 [01:31<00:46,  2.88s/it]\u001b[A\n",
      " 69% 34/49 [01:34<00:43,  2.87s/it]\u001b[A\n",
      " 71% 35/49 [01:37<00:40,  2.87s/it]\u001b[A\n",
      " 73% 36/49 [01:40<00:37,  2.87s/it]\u001b[A\n",
      " 76% 37/49 [01:43<00:34,  2.88s/it]\u001b[A\n",
      " 78% 38/49 [01:46<00:31,  2.87s/it]\u001b[A\n",
      " 80% 39/49 [01:49<00:28,  2.87s/it]\u001b[A\n",
      " 82% 40/49 [01:52<00:25,  2.87s/it]\u001b[A\n",
      " 84% 41/49 [01:54<00:22,  2.87s/it]\u001b[A\n",
      " 86% 42/49 [01:57<00:20,  2.87s/it]\u001b[A\n",
      " 88% 43/49 [02:00<00:17,  2.87s/it]\u001b[A\n",
      " 90% 44/49 [02:03<00:14,  2.87s/it]\u001b[A\n",
      " 92% 45/49 [02:06<00:11,  2.87s/it]\u001b[A\n",
      " 94% 46/49 [02:09<00:08,  2.87s/it]\u001b[A\n",
      " 96% 47/49 [02:12<00:05,  2.87s/it]\u001b[A\n",
      " 98% 48/49 [02:14<00:02,  2.88s/it]\u001b[A\n",
      "                                             \n",
      "\u001b[A{'eval_loss': 2.0590474605560303, 'eval_runtime': 140.4009, 'eval_samples_per_second': 2.785, 'epoch': 3.0}\n",
      " 60% 10581/17635 [2:06:44<1:19:30,  1.48it/s]\n",
      "100% 49/49 [02:20<00:00,  2.87s/it]\u001b[A\n",
      "{'loss': 1.6465, 'learning_rate': 5e-05, 'epoch': 3.12}\n",
      "{'loss': 1.6087, 'learning_rate': 5e-05, 'epoch': 3.26}\n",
      " 65% 11521/17635 [2:17:22<1:09:06,  1.47it/s]"
     ]
    }
   ],
   "source": [
    "!python run_clm.py \\\n",
    "--model_type gpt2-medium \\\n",
    "--model_name_or_path gpt2-medium \\\n",
    "--train_file \"train_tmp.txt\" \\\n",
    "--do_train \\\n",
    "--validation_file \"eval_tmp.txt\" \\\n",
    "--do_eval \\\n",
    "--per_gpu_train_batch_size 1 \\\n",
    "--save_steps -1 \\\n",
    "--num_train_epochs 5 \\\n",
    "--fp16 \\\n",
    "--learning_rate 5e-5 \\\n",
    "--lr_scheduler_type constant \\\n",
    "--evaluation_strategy epoch \\\n",
    "--output_dir=\"Output dir\" \\\n",
    "--overwrite_output_dir True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CpzI5mU1jPl"
   },
   "source": [
    "## Part 7: Bill Sum Text Generation using GPT-2 <a name=\"BillSumTextGeneration\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aoBsCE0W0ch",
    "outputId": "1fa9f987-22da-41d2-e216-81f3dfcfcc34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(13,), dtype=int32, numpy=\n",
       "array([ 1212,  2191,   743,   307,  9181,   355,   262,  7559, 48412,\n",
       "         284,  3961, 45097,  2191], dtype=int32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained(\"Output dir\", from_pt = True)\n",
    "\n",
    "# set the initial text to start the process of text generation\n",
    "bill_sum_input_ids = tokenizer.encode(\"This Act may be cited as the ``Farm to School Improvements Act\", return_tensors = 'tf')\n",
    "\n",
    "# print the tensor ids\n",
    "bill_sum_input_ids[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ3YNTNlBsh8"
   },
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NbzHNvvaAPns",
    "outputId": "d3cb0c22-b425-4a7d-ef26-c3140c93925a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    }
   ],
   "source": [
    "bill_sum_generated_text_samples = model.generate(\n",
    "    bill_sum_input_ids, \n",
    "    max_length = 128,  \n",
    "    num_return_sequences = 5,\n",
    "    no_repeat_ngram_size = 2,\n",
    "    repetition_penalty = 1.5,\n",
    "    top_p = 0.92,\n",
    "    temperature = 0.85,\n",
    "    do_sample = True,\n",
    "    top_k = 125,\n",
    "    early_stopping = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPdteSR_B3w1",
    "outputId": "e662b947-4962-47ae-86e7-8ddfb967801d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: This Act may be cited as the ``Farm to School Improvements Act''. [[Page 127 STAT. 1294]] (2) <<NOTE: Deadline.>> Effective date.--The amendment made by paragraph 18 of section 1346(e)(4), and in addition, all amendments making such an award under this subtitle shall take effect on September 1st 2009. SEC., 2013A--FREEDOM OF HAWTHORNE COUNTY REGISTER AND RIGHTS ASSESSMENTS WITH RESPECT TO THE PURPOSE FOR SHALLOWING A GRASS FIBERATION PROGRAMS OR ADMISSION REQUIREMENT BY RESIDENTS COMPET\n",
      "\n",
      "1: This Act may be cited as the ``Farm to School Improvements Act''. SEC. 576A--MULTIPLE DISTRIBUTION ADMINISTRATION OF SMALL, UNIFORMED AND PROFIT-DEDUCTIBLE FINANCIAL SERVICES ACT of 2015 INTELLIGENCE AUTHORITY FOR FARMING LOCATIONS WITH LAND PROGRAMS THAT DO NOT PROVIDE SUBMARINE RESEARCH FUNDS TO OTHER SCHOOL REPORTS OR CONTRACTORS AS DEFINITIONALLY NEEDY IS COMMONLY DESIGNATED BY THE PRESIDENCY GOVERNMENT ARGUMENT AFFAIR D\n",
      "\n",
      "2: This Act may be cited as the ``Farm to School Improvements Act''. (b) Comprehensive State Farm Improvement Program.--Section 1633(c)(1), referred at section 1501 of title 10, United States Code. [[Page 126 STAT.. 579]] Subtitle C--Miscellaneous Provisions SEC. 10310 - <<NOTE: 12 USC 3525.>> EROE-SHERIFF TENANGS Secs 11001 and 11202. Extension and termination of prohibition on use in agriculture that were previously prohibited under this part relating not less frequently than once each year during a calendar month or period by federal land grant agencies for\n",
      "\n",
      "3: This Act may be cited as the ``Farm to School Improvements Act''. (b) Farm Improvement- ``(1)(A)) In general.--The Secretary shall establish a program that provides incentives for farm construction in accordance with subparagraphs (B), and, if applicable under paragraph (\"G\"), by providing such incentive through grants or contracts. Such grant funding must include-- [[Page 124 STAT.2]] ($100 million); <<NOTE: Grants.>> $5.4 billion over 10 years; each of which could provide an additional 1 percent at least annually during FY 2013 alone but not more than 5 per year thereafter notwithstanding section 1454(d).\n",
      "\n",
      "4: This Act may be cited as the ``Farm to School Improvements Act of 1998''. (b) Technical and Conforming Amendments.--Section 113(a)(4), referred this section, is amended by striking ``(A)''. SEC. 10801-102.(d). <<NOTE: 12 USC 1303 note.>> Effective Date; Authority for Certain Businesses To Require Minimum Annual Wage in New York State Employees Succeeded on Jobs at Earnings Rate Not More Than $15 an Hour If The Board Has Inventor's or Authorized And Operated With A Nonresident Employer Or Smallholder Any Other Organization Required Under Sections 1087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print output for each sequence generated above\n",
    "for i, token in enumerate(bill_sum_generated_text_samples):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(token, skip_special_tokens = True)))\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Finetune GPT-2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07ac858352fc4e07a60eaa84310cc5a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dac659c527164f37b209e7aef340fb5b",
       "IPY_MODEL_c45243609ad244f3aeceb11d3e213bbc"
      ],
      "layout": "IPY_MODEL_4d112cf47cc9482fbbf8d703c10559d6"
     }
    },
    "4d112cf47cc9482fbbf8d703c10559d6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56a388a1507845bc9537b3e60d9e7477": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "684f3b3ad6284b80ac719c34e566a4b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b9dff47948a476183fc396e06b97b96": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaa17c418006450cb2596e8c263817c8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be160c60e7794714ad2bdd1a08c5b1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bf290e537239425191264f8bbfb85f80": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c45243609ad244f3aeceb11d3e213bbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aaa17c418006450cb2596e8c263817c8",
      "placeholder": "​",
      "style": "IPY_MODEL_684f3b3ad6284b80ac719c34e566a4b1",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 5.02MB/s]"
     }
    },
    "d43a635fa08a426887e5b112f73aebeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e06d435e204d46d2a900142f78a9772a",
       "IPY_MODEL_e958882ffbf4475997bff24cc1ed7229"
      ],
      "layout": "IPY_MODEL_7b9dff47948a476183fc396e06b97b96"
     }
    },
    "dac659c527164f37b209e7aef340fb5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f49cb30152974fb8a741c0c057ec365f",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_56a388a1507845bc9537b3e60d9e7477",
      "value": 1042301
     }
    },
    "e06d435e204d46d2a900142f78a9772a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed44fe544eab46778cbe6e0d02acc73b",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be160c60e7794714ad2bdd1a08c5b1a5",
      "value": 456318
     }
    },
    "e40a9cf9e1af47f29b1c97ec750b0469": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e958882ffbf4475997bff24cc1ed7229": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bf290e537239425191264f8bbfb85f80",
      "placeholder": "​",
      "style": "IPY_MODEL_e40a9cf9e1af47f29b1c97ec750b0469",
      "value": " 456k/456k [00:00&lt;00:00, 4.90MB/s]"
     }
    },
    "ed44fe544eab46778cbe6e0d02acc73b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f49cb30152974fb8a741c0c057ec365f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
